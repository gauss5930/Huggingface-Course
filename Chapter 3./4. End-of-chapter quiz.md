# End-of-chapter quiz

End-of-chapter quiz는 모두 다 적기에는 양이 많으니 각 문제 별 필요한 부분만 정리해서 작성해놓겠다. 더욱 자세한 내용을 확인하고 싶다면 https://huggingface.co/course/chapter2/8?fw=pt 를 참고하길 바란다.

### 1. emotion dataset은 감정으로 라벨링된 Twitter message를 포함하고 있다. [Hub](https://huggingface.co/datasets)에서 이를 찾아보고, 어떤 것들이 basic emotion인지 답하여라.

A: Joy, Love, Confusion, Sadness, Fear, Anger 등

### 2. [Hub](https://huggingface.co/datasets?search=emotion)에서 ar_sarcasm dataset을 찾고, 어떤 task를 지원하는지 답하여라.

A: Sentiment Classification

### 3. BERT model은 문장 쌍을 어떻게 처리하는가?

A: [CLS] sentence1 [SEP] sentence2 [SEP]

### 4. Dataset.map() method을 사용함으로써 얻는 이익은 무엇일까?

A: 함수의 결괏값이 저장되기 때문에, 이 코드를 다시 실행할 때 추가적인 시간이 들지 않는다.
다중 처리가 가능하기 때문에, 데이터셋 각각의 요소에 적용하는 것보다 훨씬 빠르다.
전체 데이터셋을 메모리에 로드하지 않고, 하나의 요소가 처리되는 즉시 결과를 저장한다.

### 5. dynamic padding은 무슨 의미일까?

A: batch가 생성되고 입력을 pad할 때, 전체 데이터셋의 가장 긴 문장의 길이에 맞춰서 padding하는 것이 아닌, 각 batch 내의 가장 긴 문장의 길이에 맞춰서 padding한다.

### 6. collate function의 목적은 무엇일까?

A: 모든 샘플을 일괄 처리해준다.

### 7. 훈련된 것과 다른 task에 해당하는 pre-trained LM으로 AutoModelFOrXxx 클래스 중 하나를 인스턴스화하면 어떻게 될까?

A: pre-trained model의 head가 사라지고, task에 적합한 새로운 head가 그 대신에 삽입된다.

### 8. TrainingArguments의 목적은?

A: Trainer와 함께 training과 evaluation에 사용되는 모든 하이퍼파라미터를 포함하고 있다.

### 9. 🤗 Accelerate 라이브러리를 사용해야 하는 이유는 무엇일까?

A: training loop가 분산 전략으로 작동하도록 해준다.
