# Processing the data

이전 챕터에서 사용한 예시를 계속 사용해보자.
다음의 코드는 PyTorch의 한 배치에서 sequence classifier을 훈련하는 방법이다.

``` python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# 이전과 동일
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# 새로운 점
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```

당연히, 단지 두 개의 문장으로 model을 학습시키는 것으로 좋은 결과를 얻어낼 수는 없다.
더 나은 성능을 얻기 위해서는, 더욱 큰 규모의 데이터셋을 준비해야할 필요가 있다.

이번 섹션에서는 MRPC(Microsoft Research Paraphrase Corpus)의 예시를 이용해 볼 것이다.
이 데이터셋은 5,801개의 문장 짝으로 이루어져 있는데, 각각의 짝들은 서로 paraphrase한 문장인지 아닌지로 라벨링이 되어있다.
이 데이터셋을 선택한 이유는 데이터셋의 크기가 그렇게 크지 않으므로, 손쉽게 실험을 진행할 수 있기 때문이다!

# Hub로부터 데이터셋 불러오기

우리의 유능한 Hub는 model 뿐만 아니라 여러 언어의 다양한 데이터셋들도 포함하고 있다.
이 다양한 데이터셋을 [여기](https://huggingface.co/datasets)에서 직접 찾아볼 수도 있다.
이 챕터를 진행하면서 이 링크에서 적당한 데이터셋을 찾아서 직접 실습을 진행해볼 수 있길 바란다.
하지만 지금은, MRPC dataset에 집중해보도록 하자!
이 데이터셋은 GLUE benchmark에 포함되어 있는 10개의 데이터셋 중 하나이다.
이 GLUE benchmark는 학술적 벤치마크로 10개의 서로 다른 text classification task에 대해서 ML model의 성능을 평가한다.

🤗 Dataset 라이브러리는 매우 간단한 명령어로 데이터셋을 불러오고 저장할 수 있도록 만들어졌다.
MRPC dataset은 다음과 같이 다운로드할 수 있다.

``` python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

``` python
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

코드의 결과에서 봐서 알 수 있듯이, DatasetDict object는 training set, validation set, test set으로 구성되어 있다.
각각은 여러가지 열(sentnece1, sentence2, label, idx)을 포함하고 있고, 각 데이터셋이 가지고 있는 데이터의 수만큼 행을 가지고 있다.
예를 들어, training set의 행은 3,668이고, 3,668개의 문장 짝을 가지고 있다는 것이다.

이 명령어는 데이터셋을 다운로드하고 저장할 수 있게 해주고, 기본값은 *~/.cache/huggingface/datasets*이다.
Chapter 2.에서 배웠던 내용을 기억해보면 저장할 폴더를 따로 직접 커스터마이즈할 수 있었다.
이 명령어에서도 HF_HOME 환경 변수를 세팅해두면 가능하다!

이렇게 불러온 데이터셋은 마치 사전처럼 indexing을 통해 특정 문장 짝을 확인할 수도 있다.

``` python
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

``` python
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

결과를 보면, 라벨은 이미 정수라는 것을 알 수 있다. 따라서 이 부분에 대해서는 따로 전처리할 필요가 없다.
정숫값이 어떤 라벨을 내포하고 있는지 확인하기 위해서, raw_train_dataset의 features를 조사하면 확인 가능하다.
다음의 코드를 실행하면 각 열의 유형을 알려준다.

``` python
raw_train_dataset.features
```

``` python
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

라벨의 유형은 ClassLabel이고, 정숫값에 따른 라벨 이름은 *names* 폴더에 저장되어 있다.
0은 not_equivalent를 의미하고, 1은 equivalent를 의미한다.

### 데이터셋 전처리

데이터셋을 전처리하기 위해서는, 모델이 이해를 할 수 있도록 text를 숫자 형태로 변환해줘야 한다.
이전의 챕터에서 봤던 것처럼, 이 과정은 tokenizer에 의해 해결된다.
tokenizer에 한 개 또는 여러 개의 문장을 집어넣게 되면, 모든 첫 번째 문장과 두 번째 문장에 대해 토큰화가 완성된다.

``` python
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

하지만, 두 개의 sequence를 model에 입력으로 넣어 두 문장의 관계가 paraphrase인지 아닌지를 파악하는 것은 불가능하다.
따라서, 두 개의 sequence를 적절한 전처리를 거쳐 하나의 쌍으로 다뤄야할 필요가 있다.
다행히도, tokenizer는 sequence 쌍을 입력으로 받아서 BERT model에 사용되는 것처럼 변환할 수 있다.

``` python
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

``` python
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

지난 챕터에서 input_ids와 attention_mask에 대해서는 얘기했었지만, token_type_ids에 대한 얘기는 뒤로 미뤘었다.
이 예시에서, token_type_ids는 어떤 문장이 첫 번째 문장이고, 어떤 문장이 두 번째 문장인지 알려주는 역할을 한다.

input_ids 내부의 ID를 다시 단어로 디코딩하면

``` python
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

다음과 같은 결과를 얻게 된다.

``` python
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

이를 통해 모델은 두 개의 문장을 입력으로 받을 때, 입력의 형식을 '[CLS] sentence1 [SEP] sentence2 [CLS]'로 가져간다는 것을 알 수 있다.
이것을 token_type_ids에 알맞게 정렬하면 다음과 같은 결과를 얻는다.

``` python
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

위의 결과에서 '[CLS] sentence1 [SEP]'에 해당하는 모든 토큰의 type은 0이고, 'sentence2 [CLS]'에 대항하는 모든 토큰의 type은 1이라는 것을 알 수 있다.

만약 다른 checkpoint를 사용했다면, 꼭 token_type_ids를 사용할 필요는 없다. 예를 들어 DistilBERT는 사용하지 않는다.
이는 모델이 pre-train 중에 이 토큰을 보았기 때문에 모델이 무엇을 해야 하는지 알아야할 때만 반환된다.

여기서, BERT는 token_type_ids와 함께 pre-train 되며 Chapter 1.에서 말했던 masked language modeling(MLM) 목표 외에 next sentence prediction이라는 부가적인 목표가 있다.
이 task의 목표는 문장 간의 관계를 파악하는 것이다.

next sentence prediction(NSP)을 통해 모델에 문장 쌍(임의로 마스킹된 토큰 포함)이 제공되고, 두 번째 문장이 첫 번째 문장 뒤에 오는지 여부를 예측하도록 한다.
task가 너무 쉽게 느껴지지 않도록, 절반 정도의 경우에는 두 문장이 기존의 문서에서 추출되게 하고, 그 외의 경우에는 두 문장이 다른 문서에서 추출되게 하였다.

일반적으로, token_type_ids가 토큰화된 입력에 있는지 없는지 걱정할 필요는 없다.
tokenizer와 model이 똑같은 chekcpoint를 쓴다면 말이다.

이제 우리의 tokenizer가 한 쌍의 문장을 처리할 수 있는지 확인하고, 전체 데이터셋에는 어떻게 적용할지 알아보도록 하자.
이전의 챕터에서처럼, tokenizer에 첫 번째 문장 목록을 주고, 그 다음에 두 번째 문장 목록을 줘서 문장 쌍 목록을 줄 수 있게 한다.
이것은 Chapter 2.에서 본 padding과 truncation 옵션과도 호환된다. 
그래서 training dataset을 한 번에 전처리하는 과정은 다음과 같다.

``` python
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

이 코드는 잘 작동하지만, dictionary 형태로 반환한다는 단점이 있다.
그리고 데이터셋에 토큰화를 진행할 때 충분한 RAM이 있어야만 잘 작동한다.

dataset을 dataset으로 보존하기 위해, Dataset.map()을 사용한다. 
이것은 만약 토큰화보다 더 많은 전처리를 필요로 한다면, 여분의 가용성을 제공해준다.
map() method는 데이터셋의 각각의 요소에 함수를 적용하는 방식으로 작동한다.
자, 이제 우리의 입력 데이터를 토큰화하는 함수를 정의해보도록 하자.

``` python
def tokenizer_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

이 함수는 dictionary를 입력으로 받아서, input_ids, attention_mask, token_type_ids를 가지고 있는 새로운 dictionary를 반환한다.
이 함수는 또한, example dictionary가 여러 개의 샘플을 가지고 있을 때도 작동하는데, 왜냐하면 tokenizer는 이전에도 봤듯이 여러 개의 문장 쌍에서도 잘 작동하기 때문이다.
이렇게 하면 map()에 대한 호출에서 batched=True 옵션을 사용할 수 있으므로 토큰화 속도가 크게 빨라진다.
tokenizer는 🤗 Tokenizers 라이브러리에서 Rust로 작성된 tokenizer의 지원을 받는다. 
이 tokenizer는 매우 빠를 수 있지만 한 번에 많은 입력을 제공하는 경우에만 가능하다.

지금은 일단 tokenization function에 padding 인수를 남겨두었다.
이것은 왜냐하면 모든 샘플을 max_length까지 패딩하는 것은 비효율적이기 때문이다.
전체 데이터 집합의 최대 길이가 아닌, 해당 배치의 최대 길이까지만 패딩을 하면 되므로 배치를 만들 때 샘플을 채우는 것이 좋다.
이러한 방식은 입력값이 매우 다양한 길이를 가질 때 많은 시간과 전력을 아껴준다!

이제 이 tokenization function을 우리의 데이터셋에 한 번에 적용시키는 방법에 대해 알아보자.
map을 진행할 때, batched=True를 사용하여, 데이터셋의 모든 요소에 대해 각각 따로 하는 것이 아닌, 한 번에 적용시켰다.
이러한 방식은 더욱 빠른 전처리를 가능하게 해준다.

``` python
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

🤗 Dataset 라이브러리가 이 처리를 적용하는 방법은 전처리 함수에서 반환된 dictionary의 각 키에 대해 하나씩 데이터셋에 새 필드를 추가하는 것이다.

``` python
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

map()과 함께 전처리 함수를 적용할 때, num_proc 변수를 함께 흘려보내면 multiprocessing도 가능하다.
하지만, 이미 🤗 Tokenizer 라이브러리는 동시에 여러 개의 샘플에 대해 토큰화가 가능하기 때문에, 자세히 다루지는 않겠다.
하지만, fast tokenizer을 사용하지 않는 상태라면, 앞서 말한 방법을 통해 전처리의 속도를 더욱 빠르게 할 수 있다.

우리가 선언한 tokenize_function을 이용하면 input_ids, attention_mask, token_type_ids를 포함하고 있는 dictionary를 반환한다. 뿐만 아니라, 3개의 필드에 대해 모두 적용된다.
tokenize_function이 map()을 적용한 데이터셋의 기존 key에 대해 새 값을 반환한 경우 기존 필드를 변경할 수도 있다.

마지막으로 해야할 것은 모든 예시들에 대해 배치를 하기 위해, 가장 긴 길이를 가진 요소의 길이만큼 모든 요소들을 패딩해야 한다.
이러한 방식의 패딩을 *dynamic padding*이라 한다.

### Dynamic padding

일괄 처리 내에서 샘플을 하나로 모으는 역할을 하는 기능을 *collate function*이라고 한다.
이 함수는 하나의 인수로 DataLoader를 지을 때 전달되는 인수이다. 
함수는 기본값으로 샘플들을 PyTorch tensor로 변환하고 이들을 합치는 역할을 수행한다.
하지만, 우리의 경우에는 이것이 불가능한데, 왜냐하면 샘플들의 크기가 다 제각각이기 때문이다.
우리는 고의적으로 padding을 뒤로 미뤘는데, 이는 각각의 배치에 따로따로 적용하기 위해서이다.
왜냐하면, 모든 배치에 한 번에 적용하면 최고로 긴 문장에 맞춰서 모든 문장이 패딩되는데, 이는 너무 비효율적이기 때문이다.
이를 통해 학습 속도를 아주 살짝 빠르게 만들어 줄 수 있는데, 만약 당신이 대용량의 TPU를 이용하고 있다면 별 상관 없는 얘기이다.

이를 실제로 진행하기 위해, 일괄 처리 하려는 데이터 집합의 항목에 올바른 양의 패딩을 적용하는 collate function을 정의해야 한다.
다행히도, 🤗 Transformer 라이브러리는 DataCollatorWithPadding을 통해 이러한 기능을 제공한다.
이것을 시행할 때 tokenizer가 필요하며 필요한 모든 작업을 수행할 수 있다.

``` python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

새로운 기능을 시험하기 위해, training set에서 약간의 샘플을 가져왔다.
불필요한 idx, sentence1, sentence2 열은 삭제하고, 문자열을 포함하여 배티의 각 항목 길이를 확인한다.


``` python
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

``` python
[50, 59, 47, 67, 59, 50, 62, 32]
```

결과를 보면 매우 다양한 길이의 샘플들이 있다는 것을 알 수 있다.
Dynamic padding을 사용하면 모든 샘플의 길이는 67로 변환되는데, 이는 배치에서 가장 긴 문장의 길이를 따르기 때문이다.
Dynamic padding이 없다면, 모든 샘플은 전체 데이터셋에서 가장 긴 문장의 길이에 맞춰서 패딩되어야 한다.
data-collator가 배치를 적절하게 dynamic padding 했는지 확인해보자.

``` python
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

``` python
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

잘 되었군요! 👍 이제, raw text에서부터 model이 다룰 수 있는 배치까지 변환하는 방법에 대해 알게 되었다.
fine-tune을 할 준비를 모두 마쳤다! 🔥
