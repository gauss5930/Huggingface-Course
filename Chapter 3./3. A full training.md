# A full training

ì´ì œ, Trainer í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì–´ë–»ê²Œ ì´ì „ ì„¹ì…˜ê³¼ ë˜‘ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ì§€ì— ëŒ€í•´ ì•Œì•„ë³´ë„ë¡ í•˜ì!
ë‹¤ì‹œ í•œ ë²ˆ ë”! ì´ ê¸€ì„ ì½ê³  ìˆëŠ” ë‹¹ì‹ ì´ ì„¹ì…˜ 2ì˜ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì„ ì´ë¯¸ ë´¤ì„ ê²ƒì´ë¼ ìƒê°í•˜ê³ , ë°ì´í„° ì „ì²˜ë¦¬ì— ëŒ€í•œ ì§§ì€ ìš”ì•½ì„ ë‚¨ê²¨ë†“ë„ë¡ í•˜ê² ë‹¤.

``` python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### í•™ìŠµì„ í•˜ê¸° ìœ„í•œ ì¤€ë¹„

ì‹¤ì œ training loopë¥¼ ì‘ì„±í•˜ê¸° ì „ì—, ëª‡ ê°œì˜ objectë¥¼ ì •ì˜í•  í•„ìš”ê°€ ìˆë‹¤.
ì²« ë²ˆì§¸ëŠ”, dataloaderë¡œ batchë“¤ì„ ë°˜ë³µí•  ë•Œ ì‚¬ìš©í•  ê²ƒì´ë‹¤.
í•˜ì§€ë§Œ, ì´ dataloaderì„ ì •ì˜í•˜ê¸° ì „ì—, Trainerê°€ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•œ ì¼ë¶€ ì‘ì—…ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ tokenizer_datasetsì— ì•½ê°„ì˜ í›„ì²˜ë¦¬ë¥¼ ì ìš©í•´ì•¼ í•œë‹¤.
ìš”ì•½í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- modelì´ ì˜ˆìƒí•˜ì§€ ì•ŠëŠ” ê°’ì„ í¬í•¨í•˜ê³  ìˆëŠ” ê°’ì— ìƒì‘í•˜ëŠ” ì—´ì„ ì‚­ì œí•¨ (sentence1ê³¼ sentence2 ê°™ì€ ì—´)
- label ì—´ì„ labelsë¡œ ê°œëª… (modelì´ labelsë¼ ì´ë¦„ ì§€ì–´ì§„ ì¸ìì— ëŒ€í•´ ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸)
- datasetì˜ í˜•ì‹ì„ ì„¤ì •í•´ì„œ list ëŒ€ì‹ ì— PyTorch tensorì„ ì¶œë ¥ìœ¼ë¡œ ë‚´ë³´ë‚´ë„ë¡ ì„¤ì •

ìš°ë¦¬ì˜ tokenizer_datasetsëŠ” ê°ê°ì˜ ë‹¨ê³„ì— ëŒ€í•´ í•˜ë‚˜ì˜ ë°©ë²•ì”©ì„ ê°€ì§€ê³  ìˆë‹¤.

``` python
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ë°›ì•„ë“¤ì¼ ìˆ˜ ìˆëŠ” ì—´ë“¤ë§Œ ë‚¨ê²Œ ëœë‹¤.

``` python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

ì´ ê³¼ì •ì´ ëë‚˜ê³  ë‚˜ë©´, ì†ì‰½ê²Œ dataloaderì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

``` python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

ë°ì´í„° ì²˜ë¦¬ ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ì—†ëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•˜ê¸° ìœ„í•´, batchë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

``` python
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

``` python
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

ì‹¤ì œ í˜•ìƒì€ ë‹¹ì‹ ì˜ ê²ƒê³¼ ì‚´ì§ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.
ì™œëƒí•˜ë©´, shuffle=Trueë¡œ ì„¤ì •í•´ë‘ê³ , ë°°ì¹˜ ë‚´ì—ì„œ ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©í•˜ê¸° ë•Œë¬¸ì— ì‹¤ì œ ëª¨ì–‘ì€ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.

ì´ì œ, ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ì™„ë²½í•˜ê²Œ ëëƒˆìœ¼ë‹ˆ, modelë¡œ ë‹¤ì‹œ ëŒì•„ê°€ë³´ì.
ì´ì „ ì„¹ì…˜ì—ì„œ í–ˆë˜ ê²ƒì²˜ëŸ¼ ì˜ˆë¥¼ ë“¤ì–´ì„œ ì„¤ëª…í•´ë³´ì•˜ë‹¤.

``` python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

í•™ìŠµ ì¤‘ì— ì•„ë¬´ëŸ° ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ batchë¥¼ ì´ modelì— ì§‘ì–´ë„£ë„ë¡ í•˜ì.

``` python
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

``` python
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

ëª¨ë“  ğŸ¤— Transformer modelë“¤ì€ labelì´ ì œê³µë˜ë©´ lossê°’ì„ ë°˜í™˜í•˜ê³ , logitê°’ ë˜í•œ ê°€ì§€ê²Œ ëœë‹¤.

ì´ì œ training loopë¥¼ ì‘ì„±í•˜ê¸° ìœ„í•œ ëª¨ë“  ì¤€ë¹„ê°€ ê±°ì˜ ëë‚¬ë‹¤!
ë”± ë‘ ê°€ì§€ë¥¼ ë†“ì³¤ëŠ”ë°, optimizerì™€ learning rate shedulerì´ë‹¤.
Trainerê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì§ì ‘ ë³µì œí•´ë³´ë©° ì•Œì•„ë³´ëŠ” ì¤‘ì´ë‹ˆ, ë˜‘ê°™ì€ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.
Trainerì— ì‚¬ìš©ë˜ëŠ” optimzerëŠ” AdamWì¸ë°, AdamWëŠ” Adamê³¼ ìœ ì‚¬í•˜ë‚˜, weight decay regularizationì—ì„œ ê¼¬ì„ì´ ë“¤ì–´ê°€ ìˆë‹¤.

``` python
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” learning rate schedulerëŠ” ìµœëŒ“ê°’(5e-5)ì—ì„œ 0ê¹Œì§€ì˜ ì„ í˜• ê°ì†Œì´ë‹¤.
ì´ë¥¼ ì ì ˆí•˜ê²Œ ì •ì˜í•˜ê¸° ìœ„í•´, ëª‡ ë²ˆì˜ training stepì„ ê±°ì¹ ì§€ ì œëŒ€ë¡œ ì•Œì•„ì•¼ í•œë‹¤.
í•œ ë§ˆë””ë¡œ, ì‹¤í–‰í•˜ë ¤ëŠ”epoch ìˆ˜ì— training batchì˜ ìˆ˜ë¥¼ ê³±í•œ ê°’ì´ë‹¤.
TrainerëŠ” 3 epochì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ê°€ì§€ê¸° ë•Œë¬¸ì—, ì´ ë˜í•œ ë˜‘ê°™ì´ í•˜ì˜€ë‹¤.

``` python
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

``` python
1377
```

### Training loop

ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ”, ë§Œì•½ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.
ì´ë¥¼ ìœ„í•´, modelê³¼ batchë¥¼ ì§‘ì–´ë„£ì„ deviceë¥¼ ì •ì˜í•´ì•¼ í•œë‹¤.

``` python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

``` python
device(type='cuda')
```

ì´ì œ í›ˆë ¨ì„ í•˜ê¸° ìœ„í•œ ëª¨ë“  ì¤€ë¹„ê°€ ëë‚¬ë‹¤!
í›ˆë ¨ì´ ì–¸ì œ ëë‚ ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ training stepì— ë”°ë¥¸ ì§„í–‰ ë°”ë¥¼ tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ì¶”ê°€í•˜ì˜€ë‹¤.

``` python
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

training loopì˜ í•µì‹¬ì´ introductionì— ìˆëŠ” ê²ƒê³¼ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
ì•„ì§ ìš°ë¦¬ëŠ” ì´ ëª¨ë¸ì—ê²Œ ì–´ë– í•œ ê¸°ë¡ì„ ë‚¨ê¸°ë„ë¡ ìš”êµ¬í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ, ì´ training loopëŠ” modelì˜ ë¹„ìš©ì— ëŒ€í•´ ì–´ë– í•œ ëŒ€ë‹µë„ ë‚´ë†“ì§€ ì•Šì„ ê²ƒì´ë‹¤.
ë”°ë¼ì„œ, evaluation loopë¥¼ ì¶”ê°€í•  í•„ìš”ê°€ ìˆë‹¤.

### Evaluation loop

ì´ì „ì— í–ˆë˜ ê²ƒì²˜ëŸ¼, ğŸ¤— Evaluation ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.
ìš°ë¦¬ëŠ” ì´ë¯¸, metric.compute() methodë¥¼ ë´¤ë˜ ì ì´ ìˆì§€ë§Œ, add_batch() methodë¥¼ ì‚¬ìš©í•˜ì—¬ prediction loopë¥¼ í†µê³¼í•  ë•Œ, metricì€ ì‹¤ì œë¡œ batchë¥¼ ì¶•ì í•  ìˆ˜ ìˆë‹¤.
batchë¥¼ ì¶•ì í–ˆë‹¤ë©´, metric.compute()ë¥¼ ì´ìš©í•˜ì—¬ ìµœì¢… ê²°ê´ê°’ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
ë‹¤ìŒì€ ì´ ëª¨ë“  ê²ƒì„ í•˜ë‚˜ë¡œ ëª¨ì€ evaluation loopì´ë‹¤!

``` python
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

``` python
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

ë‹¤ì‹œ í•œ ë²ˆ ë§í•˜ì§€ë§Œ, ê²°ê´ê°’ì´ ìš°ë¦¬ì™€ ë‹¤ë¥¼ ìˆ˜ë„ ìˆë‹¤.
ì™œëƒí•˜ë©´, model headì˜ ë¬´ì‘ìœ„ì„± ì´ˆê¸°í™”ì™€ data shuffling ë•Œë¬¸ì´ë‹¤.

### ğŸ¤— Accelerateë¥¼ ì´ìš©í•´ì„œ training loop ê°€ì†í™”í•˜ê¸°!

ì•ì„œ ì •ì˜í–ˆë˜ training loopëŠ” í•˜ë‚˜ì˜ CPU ë˜ëŠ” GPUì—ì„œë„ ì¶©ë¶„íˆ ì‘ë™í•œë‹¤.
í•˜ì§€ë§Œ, ğŸ¤— Accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ë©´, ì¡°ê¸ˆì˜ ì¡°ì •ìœ¼ë¡œë„ ì—¬ëŸ¬ ê°œì˜ GPU ë˜ëŠ” TPUì—ì„œ ë¶„ì‚° í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.
training ë° validation dataloader ìƒì„±ë¶€í„° ì‹œì‘í•˜ì—¬ training loopëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

``` python
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ê·¸ë¦¬ê³  ë‹¤ìŒì€ ë³€í™”ëœ ì ë“¤ì´ë‹¤!

``` python
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

ì¶”ê°€ëœ ì²« ë²ˆì§¸ ë¼ì¸ì€ import ë¼ì¸ì´ë‹¤.
ë‘ ë²ˆì§¸ë¡œ ì¶”ê°€ëœ ë¼ì¸ì€ Accelerator objectë¡œ í™˜ê²½ì„ ì‚´í´ë³´ê³ , ì ì ˆí•œ ë¶„ì‚° ì„¤ì •ì„ ì´ˆê¸°í™”í•œë‹¤.
ğŸ¤— AcceleratorëŠ” device placementë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ ì¥ì¹˜ì— modelì„ ë°°ì¹˜í•˜ëŠ” ë¼ì¸ì„ ì œê±°í•  ìˆ˜ ìˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì€ dataloader, model ë° optimizerë¥¼ accelerator.prepare()ë¡œ ë³´ë‚´ëŠ” ë¼ì¸ì—ì„œ ìˆ˜í–‰ëœë‹¤.
ì´ê²ƒì€ ë¶„ì‚° í•™ìŠµì´ ì˜ë„í•œ ëŒ€ë¡œ ì˜ ë˜ë„ë¡ í•˜ê¸° ìœ„í•´, í•´ë‹¹ objectë¥¼ ì ì ˆí•œ ì»¨í…Œì´ë„ˆì— ë˜í•‘í•œë‹¤.
ë‚˜ë¨¸ì§€ ë³€ê²½ ì‚¬í•­ì€ deviceì— batchë¥¼ ë°°ì¹˜í•˜ëŠ” ë¼ì¸ì„ ì œê±°í•˜ê³ , loss.backward()ë¥¼ accelerator.bachward(loss)ë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ë‹¤.

ë‹¤ìŒì€ ğŸ¤— Acceleratorë¥¼ ì´ìš©í•´ì„œ ì™„ì„±ì‹œí‚¨ training loopì´ë‹¤!

``` python
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ì´ ì½”ë“œë¥¼ train.py ìŠ¤í¬ë¦½íŠ¸ì— ë„£ì–´ì£¼ë©´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì–´ë– í•œ ì¢…ë¥˜ì˜ ë¶„ì‚° ì…‹íŒ…ì´ë“  ì‘ë™í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ì¤„ ê²ƒì´ë‹¤.
ë‹¹ì‹ ì˜ ë¶„ì‚° ì…‹íŒ…ì—ì„œ ì‹œë„í•´ë³´ë ¤ë©´, ë‹¤ìŒì˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ë¼.

``` python
accelerate config
```

ê·¸ëŸ¬ë©´ ëª‡ ê°€ì§€ ì§ˆë¬¸ì— ë‹µí•˜ê³  ì´ ëª…ë ¹ì—ì„œ ì‚¬ìš©í•˜ëŠ” êµ¬ì„± íŒŒì¼ì— ë‹µì„ dumpí•˜ë¼ëŠ” ë©”ì‹œì§€ê°€ í‘œì‹œëœë‹¤.

``` python
accelerate launch train.py
```

ì´ë ‡ê²Œ í•´ì„œ ë¶„ì‚° í•™ìŠµì´ ì‹œì‘ëœë‹¤!

ì´ê²ƒì„ Colabê³¼ ê°™ì€ Notebookì— ì‹¤í–‰í•˜ë ¤ë©´, ê·¸ì € training_function()ì˜ ì½”ë“œë¥¼ ë¶™ì—¬ë„£ê³ , ë§ˆì§€ë§‰ ì…€ì„ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰í•˜ë©´ ëœë‹¤.

``` python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```
