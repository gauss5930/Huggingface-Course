# pipeline, ê·¸ ì•ˆì—ì„œëŠ” ë¬´ìŠ¨ ì¼ì´?

ì™„ì„±ëœ ì˜ˆì‹œë¥¼ í•˜ë‚˜ ë³´ê³  ì‹œì‘í•˜ì. 
Chapter 1.ì—ì„œ ìˆ˜í–‰í–ˆë˜ ì½”ë“œì˜ ë‚´ë¶€ì—ì„œëŠ” ì–´ë–¤ ê³¼ì •ì´ ë°œìƒí•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì.

``` python
!pip install datasets evaluate transformers[sentencepiece]

from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

ìœ„ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ê²Œ ëœë‹¤.

``` python
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Chapter 1.ì—ì„œ ë´¤ë˜ ê²ƒì²˜ëŸ¼, ì´ pipeline ê·¸ë£¹ì€ 3 ê°œì˜ stepìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. 
ë°”ë¡œ ì „ì²˜ë¦¬, ì…ë ¥ì„ ëª¨ë¸ë¡œ í˜ë ¤ë³´ë‚´ê¸°, í›„ì²˜ë¦¬ ì‘ì—…ìœ¼ë¡œ ë§ì´ë‹¤.

![full_nlp_pipeline](https://user-images.githubusercontent.com/80087878/218010695-a8f283f3-b308-4f69-bcfb-fde785526aba.svg)

### tokenizerë¡œ ì „ì²˜ë¦¬

ë‹¤ë¥¸ ì‹ ê²½ë§ ëª¨ë¸ë“¤ì²˜ëŸ¼, TransformerëŠ” raw textë¥¼ ë°”ë¡œ ì²˜ë¦¬í•  ìˆ˜ ì—†ë‹¤. 
ê·¸ë˜ì„œ, pipelineì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” text inputì„ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì´ë‹¤.
ì´ ì‘ì—…ì„ ìœ„í•´ *tokenizer*ë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤:

- ì…ë ¥ì„ *token*ì´ë¼ ë¶€ë¥´ëŠ” word, subword, ê¸°í˜¸ ë“±ìœ¼ë¡œ ë‚˜ëˆ”
- ê°ê°ì˜ tokenì„ ìˆ«ìì— ë§¤í•‘
- ëª¨ë¸ì— ìœ ìš©í•´ ë³´ì´ëŠ” ì¶”ê°€ì ì¸ inputì„ ì¶”ê°€

ëª¨ë“  ì „ì²˜ë¦¬ëŠ” ëª¨ë¸ì´ pretrainë  ë•Œ ì´ìš©ëœ ë°©ë²•ê³¼ ë˜‘ê°™ê²Œ ì§„í–‰ë˜ì–´ì•¼ í•œë‹¤. 
ê·¸ë˜ì„œ, Model Hubì—ì„œ ì •ë³´ë¥¼ ë¨¼ì € ì–»ì–´ì™€ì•¼ í•œë‹¤.
ì´ë¥¼ ìœ„í•´ì„œ, AutoTokenizerì˜ from_pretrained()ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
ëª¨ë¸ì˜ checkpoint nameì„ ì‚¬ìš©í•´ì„œ, ìë™ìœ¼ë¡œ ëª¨ë¸ì˜ tokenizer ë°ì´í„°ë¥¼ ì°¾ê³  ìºì‹œí•´ë‘”ë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, 'distilbert-base-uncased-finetuned-sst-2-english'ì˜ tokenizerì„ ì°¾ê³  ì‹¶ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•˜ë©´ ëœë‹¤.

``` python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

ì´ë ‡ê²Œ í•´ì„œ ì–»ì–´ì§„ dictionaryë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” tensorë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤.
ë³€í™˜í•˜ê³  ì‹¶ì€ tensorì˜ ì¢…ë¥˜ì— ë”°ë¼ 'return_tensors'ë¥¼ ìˆ˜ì •í•˜ì—¬ ì›í•˜ëŠ” ê°’ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

``` python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

ìœ„ ì½”ë“œì—ì„œëŠ” PyTorch tensorë¥¼ ì¶œë ¥ìœ¼ë¡œ ë°›ê²Œ ë˜ëŠ”ë°, ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

``` python
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

ì¶œë ¥ê°’ì€ 'input_ids'ì™€ 'attention_mask'ì˜ ë‘ ê°€ì§€ keyë¥¼ ê°–ëŠ” dictionaryì´ë‹¤. 
'input_ids'ëŠ” ê°ê°ì˜ ë¬¸ì¥ì— ëŒ€í•œ ìˆ«ì í–‰ì„ ê°€ì§„ë‹¤. (ì§€ê¸ˆì˜ ì˜ˆì‹œì—ì„œëŠ” 2ê°œì˜ í–‰ì„ ê°€ì§)
'attention_mask'ì— ëŒ€í•´ì„œëŠ” ë‚˜ì¤‘ì˜ chapterì—ì„œ ì„¤ëª…í•  ê²ƒì´ë‹¤.

### ëª¨ë¸ì— ì ìš©ì‹œì¼œ ë³´ì!

tokenizerì„ ë‹¤ìš´ë¡œë“œí•œ ê²ƒì²˜ëŸ¼ pretrained model ë˜í•œ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆë‹¤. 
ğŸ¤— TransformerëŠ” AutoModel í´ë˜ìŠ¤ë¥¼ ì œê³µí•˜ê³ , ì—¬ê¸° ì•ˆì—ëŠ” from_pretrained()ê°€ ìˆë‹¤.

``` python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```

ì´ architectureëŠ” ê¸°ë³¸ Transformer moduleë§Œ í¬í•¨í•˜ê³  ìˆë‹¤. 
ê° ëª¨ë¸ ì…ë ¥ì— ëŒ€í•´ Transformer ëª¨ë¸ì˜ í•´ë‹¹ ì…ë ¥ì— ëŒ€í•œ context ì´í•´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ ê²€ìƒ‰í•œë‹¤.

ì´ëŸ¬í•œ hidden stateëŠ” ê·¸ ìì‹ ë§Œìœ¼ë¡œë„ ìœ ìš©í•œë°, ì´ë“¤ì€ ë³´í†µ ëª¨ë¸ì˜ ë‹¤ë¥¸ íŒŒíŠ¸ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤. *head*ë¡œ ì•Œë ¤ì ¸ìˆëŠ” ê²ƒì²˜ëŸ¼ ë§ì´ë‹¤.

Chapter 1.ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ taskì„ì—ë„ ë˜‘ê°™ì€ architectureì—ì„œ ìˆ˜í–‰ë  ìˆ˜ ìˆì§€ë§Œ, ê°ê°ì˜ taskì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ headë¥¼ ê°€ì§„ë‹¤ê³  ì„¤ëª…í•˜ì˜€ë‹¤.

### ê³ ì°¨ì› ë²¡í„°?

Transformerë¡œë¶€í„° ì¶œë ¥ë˜ëŠ” ë²¡í„°ëŠ” ë³´í†µ ë§¤ìš° í¬ë‹¤. ë³´í†µì€ ë‹¤ìŒê³¼ ê°™ì´ 3ê°œì˜ ì°¨ì›ì„ ê°–ëŠ”ë‹¤.

- **Batch Size**: í•œ ë²ˆì— ì²˜ë¦¬ëœ sequenceì˜ ìˆ˜(ì˜ˆì‹œì—ì„œëŠ” 2ê°œ)
- **Sequence Length**: sequenceì˜ numerical representationì˜ ê¸¸ì´
- **Hidden Size**: ê° ëª¨ë¸ ì…ë ¥ì˜ ë²¡í„° ì°¨ì›

Transformerì˜ ì¶œë ¥ ë²¡í„°ê°€ 'ê³ ì°¨ì› ë²¡í„°ë¼ê³  ë¶ˆë¦¬ëŠ” ì´ìœ ëŠ” ë§ˆì§€ë§‰ ê°’ ë•Œë¬¸ì´ë‹¤. 
hidden sizeê°€ ë§¤ìš° ì»¤ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. (768ë¶€í„° 3072ì— ì´ë¥´ê¸°ê¹Œì§€ ë§¤ìš° ê´‘ë²”ìœ„í•¨)

ì „ì²˜ë¦¬í•œ ì…ë ¥ì„ ëª¨ë¸ì— ì œê³µí•˜ë©´ ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

``` python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)

torch.Size([2, 16, 768])
```

# Model heads: ìˆ«ì ì´í•´í•˜ê¸°

modelì˜ headëŠ” hidden stateì˜ ê³ ì°¨ì› ë²¡í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³ , ë‹¤ë¥¸ ì°¨ì›ìœ¼ë¡œ projectí•œë‹¤. 
ì´ë“¤ì€ ë³´í†µ í•œ ê°œ ë˜ëŠ” ë‘ ê°œì˜ ì„ í˜• ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.

![transformer_and_head](https://user-images.githubusercontent.com/80087878/218017260-01f56e37-dc8a-4062-8885-5f02cbade1bd.svg)

Transformer modelì˜ ì¶œë ¥ì€ ì²˜ë¦¬ë˜ê¸° ìœ„í•´ ë°”ë¡œ model headë¡œ ì „ì†¡ëœë‹¤.

ì´ ì‚¬ì§„ì—ì„œ ëª¨ë¸ì€ ì„ë² ë”© ë ˆì´ì–´ì™€ í›„ì† ë ˆì´ì–´ë¡œ í‘œì‹œë‹¤. 
ì„ë² ë”© ë ˆì´ì–´ëŠ” í† í°í™”ëœ ì…ë ¥ì˜ ê° ì…ë ¥ IDë¥¼ ì—°ê²°ëœ í† í°ì„ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤. 
í›„ì† ë ˆì´ì–´ëŠ” attention ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ë²¡í„°ë¥¼ ì¡°ì‘í•˜ì—¬ ë¬¸ì¥ì˜ ìµœì¢… í‘œí˜„ì„ ìƒì„±í•œë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ì„œ, sequence classification headê°€ ìˆëŠ” modelì´ í•„ìš”í•˜ë‹¤ê³  ì³ë³´ì. 
AutoModel í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ëŠ” ì•Šì§€ë§Œ, AutoModelForSequenceClassificationì„ ì‚¬ìš©í•œë‹¤!

``` python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```

ì¶œë ¥ì„ ì‚´í´ë³´ë©´, ì°¨ì›ì´ í˜„ì ¸íˆ ë‚®ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 
model headëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ ë°›ì•„ë“¤ì—¬ì„œ, ë‘ ê°œì˜ ê°’ì„ í¬í•¨í•˜ëŠ” ë²¡í„°ë¥¼ ì¶œë ¥í•œë‹¤. (ê° labelë‹¹ í•˜ë‚˜)

``` python
print(outputs.logits.shape)

torch.Size([2, 2])
```

ë‘ ê°œì˜ ë¬¸ì¥ê³¼ ë‘ ê°œì˜ labelì„ ê°€ì§€ê³  ìˆì—ˆê¸°ì—, modelì˜ ì¶œë ¥ì€ 2 x 2ì˜ í˜•íƒœë¥¼ ë„ëŠ” ê²ƒì´ë‹¤.

### ì¶œë ¥ í›„ì²˜ë¦¬

modelì˜ ì¶œë ¥ìœ¼ë¡œ ì–»ê²Œ ëœ ê°’ì€ ê·¸ ìì‹ ë§Œìœ¼ë¡œëŠ” ì–´ë– í•œ ì˜ë¯¸ë¥¼ ë‚´í¬í•˜ê³  ìˆì§€ëŠ” ì•Šë‹¤. 
ë”ìš± ìì„¸íˆ ì‚´í´ë³´ì.

``` python
print(outputs.logits)

tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```

ì´ë ‡ê²Œ ì–»ì–´ì§„ ê°’ë“¤ì€ í™•ë¥ ê°’ì´ ì•„ë‹ˆë¼, ì „ë¶€ *logit*ì´ë‹¤. 
ì´ *logit*ì€ ê°€ê³µë˜ì§€ ì•Šì€, ê·¸ì € ë”± ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ë¥¼ ê±°ì³ì„œ ë°”ë¡œ ë‚˜ì˜¨ ê°’ì´ë‹¤.
ì´ ê°’ì„ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ì„œëŠ” SoftMax layerì„ ê±°ì³ì•¼ í•œë‹¤.

``` python
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)

tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```

ì´ë ‡ê²Œ í™•ë¥ ê°’ì„ ì–»ê²Œ ë˜ì—ˆê³ , id2labelì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ labelì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

``` python
model.config.id2label

{0: 'NEGATIVE', 1: 'POSITIVE'}
```

ì´ë ‡ê²Œ í•´ì„œ pipelineì˜ 3ê°€ì§€ ë‹¨ê³„ì¸ tokenizerë¡œ ì „ì²˜ë¦¬, ì…ë ¥ì„ ëª¨ë¸ë¡œ í˜ë¦¬ê¸°, í›„ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤. 
ì´ì œ ê°ê°ì˜ ë”˜ê³„ì— ëŒ€í•´ ë”ìš± ìì„¸íˆ ì•Œì•„ë³´ë„ë¡ í•˜ì!
