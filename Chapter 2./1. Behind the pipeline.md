# pipeline, 그 안에서는 무슨 일이?

완성된 예시를 하나 보고 시작하자. 
Chapter 1.에서 수행했던 코드의 내부에서는 어떤 과정이 발생하는지 확인해보자.

``` python
!pip install datasets evaluate transformers[sentencepiece]

from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

위의 코드를 실행하면, 다음과 같은 결과를 얻게 된다.

``` python
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

Chapter 1.에서 봤던 것처럼, 이 pipeline 그룹은 3 개의 step으로 구성되어 있다. 
바로 전처리, 입력을 모델로 흘려보내기, 후처리 작업으로 말이다.

![full_nlp_pipeline](https://user-images.githubusercontent.com/80087878/218010695-a8f283f3-b308-4f69-bcfb-fde785526aba.svg)

### tokenizer로 전처리

다른 신경망 모델들처럼, Transformer는 raw text를 바로 처리할 수 없다. 
그래서, pipeline의 첫 번째 단계는 text input을 모델이 이해할 수 있도록 숫자로 변환하는 작업이다.
이 작업을 위해 *tokenizer*를 사용하는데, 이는 다음과 같은 작업을 수행한다:

- 입력을 *token*이라 부르는 word, subword, 기호 등으로 나눔
- 각각의 token을 숫자에 매핑
- 모델에 유용해 보이는 추가적인 input을 추가

모든 전처리는 모델이 pretrain될 때 이용된 방법과 똑같게 진행되어야 한다. 
그래서, Model Hub에서 정보를 먼저 얻어와야 한다.
이를 위해서, AutoTokenizer의 from_pretrained()을 사용하였다.
모델의 checkpoint name을 사용해서, 자동으로 모델의 tokenizer 데이터를 찾고 캐시해둔다.
예를 들어, 'distilbert-base-uncased-finetuned-sst-2-english'의 tokenizer을 찾고 싶으면 다음과 같이 사용하면 된다.

``` python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

이렇게 해서 얻어진 dictionary를 입력으로 사용하기 위해서는 tensor로 변환해야 한다.
변환하고 싶은 tensor의 종류에 따라 'return_tensors'를 수정하여 원하는 값을 얻을 수 있다.

``` python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

위 코드에서는 PyTorch tensor를 출력으로 받게 되는데, 결과는 다음과 같다.

``` python
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

출력값은 'input_ids'와 'attention_mask'의 두 가지 key를 갖는 dictionary이다. 
'input_ids'는 각각의 문장에 대한 숫자 행을 가진다. (지금의 예시에서는 2개의 행을 가짐)
'attention_mask'에 대해서는 나중의 chapter에서 설명할 것이다.

### 모델에 적용시켜 보자!

tokenizer을 다운로드한 것처럼 pretrained model 또한 같은 방식으로 다운로드할 수 있다. 
🤗 Transformer는 AutoModel 클래스를 제공하고, 여기 안에는 from_pretrained()가 있다.

``` python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```

이 architecture는 기본 Transformer module만 포함하고 있다. 
각 모델 입력에 대해 Transformer 모델의 해당 입력에 대한 context 이해를 나타내는 고차원 벡터를 검색한다.

이러한 hidden state는 그 자신만으로도 유용한데, 이들은 보통 모델의 다른 파트에 입력으로 사용된다. *head*로 알려져있는 것처럼 말이다.

Chapter 1.에서 서로 다른 task임에도 똑같은 architecture에서 수행될 수 있지만, 각각의 task에 대해 서로 다른 head를 가진다고 설명하였다.

### 고차원 벡터?

Transformer로부터 출력되는 벡터는 보통 매우 크다. 보통은 다음과 같이 3개의 차원을 갖는다.

- **Batch Size**: 한 번에 처리된 sequence의 수(예시에서는 2개)
- **Sequence Length**: sequence의 numerical representation의 길이
- **Hidden Size**: 각 모델 입력의 벡터 차원

Transformer의 출력 벡터가 '고차원 벡터라고 불리는 이유는 마지막 값 때문이다. 
hidden size가 매우 커질 수 있기 때문이다. (768부터 3072에 이르기까지 매우 광범위함)

전처리한 입력을 모델에 제공하면 이를 확인할 수 있다.

``` python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)

torch.Size([2, 16, 768])
```

# Model heads: 숫자 이해하기

model의 head는 hidden state의 고차원 벡터를 입력으로 받고, 다른 차원으로 project한다. 
이들은 보통 한 개 또는 두 개의 선형 레이어로 구성되어 있다.

![transformer_and_head](https://user-images.githubusercontent.com/80087878/218017260-01f56e37-dc8a-4062-8885-5f02cbade1bd.svg)

Transformer model의 출력은 처리되기 위해 바로 model head로 전송된다.

이 사진에서 모델은 임베딩 레이어와 후속 레이어로 표시다. 
임베딩 레이어는 토큰화된 입력의 각 입력 ID를 연결된 토큰을 나타내는 벡터로 변환한다. 
후속 레이어는 attention 메커니즘을 사용하여 이러한 벡터를 조작하여 문장의 최종 표현을 생성한다.

예를 들어서, sequence classification head가 있는 model이 필요하다고 쳐보자. 
AutoModel 클래스를 사용하지는 않지만, AutoModelForSequenceClassification을 사용한다!

``` python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```

출력을 살펴보면, 차원이 현져히 낮다는 것을 알 수 있다. 
model head는 고차원 벡터를 받아들여서, 두 개의 값을 포함하는 벡터를 출력한다. (각 label당 하나)

``` python
print(outputs.logits.shape)

torch.Size([2, 2])
```

두 개의 문장과 두 개의 label을 가지고 있었기에, model의 출력은 2 x 2의 형태를 띄는 것이다.

### 출력 후처리

model의 출력으로 얻게 된 값은 그 자신만으로는 어떠한 의미를 내포하고 있지는 않다. 
더욱 자세히 살펴보자.

``` python
print(outputs.logits)

tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```

이렇게 얻어진 값들은 확률값이 아니라, 전부 *logit*이다. 
이 *logit*은 가공되지 않은, 그저 딱 모델의 마지막 레이어를 거쳐서 바로 나온 값이다.
이 값을 확률값으로 변환하기 위해서는 SoftMax layer을 거쳐야 한다.

``` python
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)

tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```

이렇게 확률값을 얻게 되었고, id2label을 사용하여 예측 label을 얻을 수 있다.

``` python
model.config.id2label

{0: 'NEGATIVE', 1: 'POSITIVE'}
```

이렇게 해서 pipeline의 3가지 단계인 tokenizer로 전처리, 입력을 모델로 흘리기, 후처리를 진행하였다. 
이제 각각의 딘계에 대해 더욱 자세히 알아보도록 하자!
