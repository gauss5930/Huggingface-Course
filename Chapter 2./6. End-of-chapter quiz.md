# End-of-chapter quiz

End-of-chapter quiz는 모두 다 적기에는 양이 많으니 각 문제 별 필요한 부분만 정리해서 작성해놓겠다.
더욱 자세한 내용을 확인하고 싶다면 https://huggingface.co/course/chapter2/8?fw=pt 를 참고하길 바란다.

### 1. language modeling pipeline의 순서는 어떻게 될까?

A: tokenizer가 text와 return ID를 다룬다. 모델은 이러한 ID를 이용해서 예측을 내놓는다.
tokenizer는 다시 한 번 사용되어 이러한 예측을 다시 text로 변환한다.

### 2. 기본 Transformer 모델에 의한 tensor 출력의 차원은 몇 개이며 각각 무엇일까?

A: 3. 각각 sequence length, batch size, hidden size이다.

### 3. subword toeknization의 예시는 무엇일까?

A: WordPiece, BPE, Unigram 등

### 4. model head는 무엇일까?

A: 추가적인 요소로, 보통 한 개에서 여러 개의 레이어로 이루어져 있다. model head는 transformer의 예측을 task-specific한 출력으로 변환한다.

### 5. AutoModel이 무엇일까?

A: checkpoint에 알맞는 architecture을 반환해주는 object

### 6. 길이가 다른 sequence를 배치화할 때 알아야 할 기술들은 무엇일까?

A: padding이 필요하다. 왜냐하면 tensor는 정사각형 모양으로만 가능하기 때문에, padding을 통해 이를 맞춰줘야 한다.
그리고 truncating도 필요하다. 만약 max length가 문장의 길이보다 길다면 줄여야 하기 때문이다.
마지막으로 Attention masking도 필요하다.

### 7. sequence classification model에 의한 logit 출력에 SoftMax 함수를 적용하는 요점은 무엇일까?

A: 상한과 하한선을 적용해서 이해할 수 있도록 만들어 준다. 출력의 총합은 1이 되게 되고, 이것은 확률 분포로 나타나게 된다.

### 8. 대부분의 tokenizer API는 어떤 방식으로 작동되는가?

A: tokenizer object를 불러올 수 있다.

### 9. 다음의 코드 샘플에서 result 변수가 가지는 값은 무엇일까?

``` python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```

A: 문자열의 리스트를 갖게 된다. 각각의 문자열은 토큰이다.

### 10. 다음의 코드에서 잘못된 점이 있을까?

``` python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```

A: tokenizer와 model은 항상 같은 checkpoint를 가져야 한다!
