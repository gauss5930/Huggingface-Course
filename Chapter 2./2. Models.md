# Models

이번 절에서는 모델을 생성하고 사용하는 방법에 대해 자세히 다루어 보겠다. 
이를 위해 checkpoint로부터 손쉽게 사용할 수 있는 *Automodel*을 사용할 것이다.

*Automodel* 클래스와 이와 관련된 모든 클래스는 라이브러리에서 사용 가능한 광범위한 모델을 모아놓은 간단한 집합소 느낌이다.
이 집합소는 현명하기 때문에, 나의 checkpoint에 적합한 model architecture을 자동으로 생각해서 찾아내고, 이 architecture로 model을 작동한다.

하지만, 내가 어떤 모델을 사용해야할 지 알고 있다면, 그 architecture을 정의해서 사용할 수 있다.
이 클래스가 어떻게 작동하는지 BERT model을 이용하여 직접 확인해보자!

### Transformer 생성

BERT model을 사용하기 위해 처음으로 해야할 것은 구성 객체(configuration object)를 불러오는 것이다.

``` python
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)
```

구성 객체는 여러가지 요소들을 가지고 있는데, 이 요소들은 모델을 만들기 위해 사용된 요소들이다.

``` python
print(config)
```

``` python
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

만약 이 값들을 한 번도 본 적이 없다면, 이들에 대해 익숙해질 필요가 있다. 
- *hidden_size*: hidden_states 벡터의 크기를 정의
- *num_hidden_layers*: Transformer model이 가지는 레이어의 수 정의

### 다른 로딩 방법

기본 구성에서 모델을 생성하게 되면 임의의 값으로 초기화되게 된다.

``` python
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Model is randomly initialized!
```

모델은 이 상태에서도 사용될 수 있지만, 아마도 횡설수설한 대답을 내놓을 것이다. 
이를 방지하기 위해서, 먼저 학습되어야 한다.
그러기 위해서는, 모델을 처음부터 학습시켜야 하는데, Chapter 1. 에서 봐서 알 수 있듯이, 이 과정은 많은 양의 시간과 데이터를 소모한다.
이렇게 불필요하고, 중복되는 노력을 줄이기 위해, 이미 학습되어 있는 모델을 반드시 재사용하고 공유해야 한다.

사전에 학습된 Transformer을 불러오는 것은 매우 간단한데, from_pretrained()를 사용하면 된다.

``` python
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

이전에 봤던 것처럼, BertModel을 다른 동등한 AutoModel 클래스로 대체할 수도 있다. 
위의 코드 샘플에서는 BertConfig를 사용하지 않았다.
그 대신에, bert-base-cased를 사용하여 사전 학습된 모델을 불러왔다.
이것은 BERT의 저자가 직접 학습한 모델 체크포인트이다.

이 모델은 체크포인트의 모든 가중치로 초기화되어 있다.
따라서 이 모델은 모델이 학습된 task에 대해서는 바로 추론을 진행할 수 있고, 새로운 task에 대해서는 fine-tune될 수 있다.
밑바닥부터 가중치를 학습시키는 것보다 더욱 빠르게 좋은 결과를 얻어낼 수 있다!

### 저장 방법

모델을 저장하는 것은 불러오는 것만큼이나 쉽다.
save_pretrained() method를 사용하면 되는데, 이는 from_pretrained() method와 유사하다.

``` python
model.save_pretrained("directory_on_my_computer")
```

이렇게 하면 두 개의 파일이 내 디스크에 저장된다.

``` python
ls directory_on_my_computer

config.json pytorch_model.bin
```

congif.json 파일을 보면, model architecture을 만들기 위해 필요한 요소들을 알 수 있다.
이 파일은 메타데이터 또한 포함하고 있는데, 이것은 이 모델이 어떤 체크포인트로부터 비롯되었는지와 🤗 Transformer의 버전을 저장하고 있다.

pytorch.bin 파일은 state dictionary로, 모델의 모든 가중치 값을 포함하고 있다.
구성은 나의 모델 architecture을 이해하기 위해 필요하고, model weights는 model의 parameter를 의미한다.

### 추론을 위해 Transformer 사용하기

이제 어떻게 모델을 불러오고 저장하는지 알게 됐으니, 이를 사용하여 몇 가지 예측을 해보자!
Transformer 모델은 오직 Tokenizer가 생성해낸 숫자만을 처리할 수 있다.
하지만 Tokenizer에 대해 얘기하기 전에, model이 받아들이는 입력에 대해 얘기해보자.

Tokenizer는 적절한 프레임워크의 텐서에 입력을 캐스팅할 수 있지만, 좀 더 자세한 이해를 위해, 모델에 입력을 보내기 전에 수행해야 할 작업을 간단히 살펴보겠다.

몇 가지의 sequence가 있다고 가정해보자.

``` python
sequences = ["Hello!", "Cool.", "Nice!"]
```

Tokenizer는 이들을 보통 *Input IDs*로 불리는 vocabulary index로  변환한다. 
각각의 sequence는 숫자 리스트로 변환되어 있다! 결과는 다음과 같다.

``` python
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

이 리스트는 인코딩된 sequence이다. 
Tensor는 오직 정사각형 모양만 받아들인다 (행렬을 생각하면 이해하기 쉽다).
이 "배열"은 이미 정사각형 모양이기 때문에, 이를 tensor로 변환하는 것은 어렵지 않다.

``` python
import torch

model_inputs = torch.tensor(encoded_sequences)
```

### tensor을 model의 입력으로 사용하기

tensor을 모델의 입력으로 사용하는 것은 매우 간단하다! 그냥 모델을 입력과 함께 선언하면 된다!

``` python
output = model(model_inputs)
```

model은 정말 많은 서로 다른 인자들을 받아들지만, 그중에서 input IDs는 필수적이다.
어떤 인자가 중요하고, 꼭 필요한지는 나중에 다루기로 하고, 그보다 Transformer의 입력을 만들어주는 tokenizer에 대해 더욱 자세하게 알아보도록 하겠다.
