# Handling multiple sequences

ì´ì „ ì ˆì—ì„œëŠ” ì§§ì€ í•œ ë¬¸ì¥ ê°™ì€ ê°„ë‹¨í•œ ì˜ˆë¥¼ ì‚´í´ë³´ì•˜ë‹¤.
í•˜ì§€ë§Œ, ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ë“¤ì´ ë– ì˜¤ë¥¼ ê²ƒì´ë‹¤.
- ì—¬ëŸ¬ ê°œì˜ sequenceë¥¼ ì–´ë–»ê²Œ ë‹¤ë¤„ì•¼ í• ê¹Œ?
- ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´ì˜ ì—¬ëŸ¬ ê°œì˜ sequenceë¥¼ ì–´ë–»ê²Œ ë‹¤ë¤„ì•¼ í• ê¹Œ?
- vocabulary indexëŠ” ê³¼ì—° ëª¨ë¸ì´ ê°€ì¥ ì˜ ì‘ë™í•˜ë„ë¡ í—ˆìš©í•˜ëŠ” ìœ ì¼í•œ ì…ë ¥ì¼ê¹Œ?
- ë„ˆë¬´ ê¸´ ê¸¸ì´ì˜ sequenceì— ëŒ€í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?

ì´ëŸ¬í•œ ì§ˆë¬¸ë“¤ì´ ì‹¤ì œë¡œ ë¬¸ì œë¡œ ë‚˜íƒ€ë‚¬ì„ ë•Œ, ì–´ë–»ê²Œ í•´ê²°í•´ì•¼ í•  ì§€ ğŸ¤— Transformer APIë¥¼ ì´ìš©í•˜ì—¬ í•´ê²°í•´ë³´ì!

### Modelì€ ì…ë ¥ ë°°ì¹˜ë¥¼ ì˜ˆìƒ

ì´ì „ ì˜ˆì œì—ì„œ sequenceê°€ ì–´ë–»ê²Œ ìˆ«ì ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ë˜ëŠ”ì§€ ì‚´í´ë³´ì•˜ë‹¤. 
ì´ì œ ì´ ìˆ«ì ë¦¬ìŠ¤íŠ¸ë¥¼ tensorë¡œ ë³€í™˜í•´ì„œ ëª¨ë¸ì— ì£¼ì–´ë³´ë„ë¡ í•˜ì!

``` python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```

``` python
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```

ì˜¤ë¥˜ê°€ ë°œìƒí–ˆë‹¤! ì™œ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê±¸ê¹Œ? section 2ì˜ pipelineì˜ stepì„ ë”°ë¼ê°€ë³´ë„ë¡ í•˜ì.

ë¬¸ì œëŠ” ëª¨ë¸ì— í•˜ë‚˜ì˜ sequenceë§Œì´ ë“¤ì–´ê°”ê¸° ë•Œë¬¸ì´ë‹¤.
ğŸ¤— TransformerëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ì„ ë°›ì•„ë“¤ì´ë„ë¡ ì„¤ì •ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²ƒì´ë‹¤.
ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” tokenizerë¥¼ sequenceì— ì ìš©í–ˆì„ ë•Œ ë°±ê·¸ë¼ìš´ë“œì—ì„œ tokenizerê°€ ìˆ˜í–‰í•˜ëŠ” ëª¨ë“  ì‘ì—…ì„ ìˆ˜í–‰í•˜ë ¤ê³  í–ˆë‹¤.
í•˜ì§€ë§Œ ìì„¸íˆ ë“¤ì—¬ë‹¤ë³´ë©´, tokenizerëŠ” input IDsë¥¼ ê·¸ëƒ¥ tensorë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê·¸ ìœ„ì— í•˜ë‚˜ì˜ ì°¨ì›ì„ ë” ì¶”ê°€í•œë‹¤.

``` python
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

``` python
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```

ìƒˆë¡œìš´ ì°¨ì›ì„ ì¶”ê°€í•˜ì—¬ ë‹¤ì‹œ ì‹œë„í•´ë³´ì!

``` python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])   #idsë¥¼ ë°°ì—´ í˜•íƒœë¡œ ì§‘ì–´ë„£ìŒ
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```

ì¶œë ¥ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ì˜¤ê²Œ ëœë‹¤!

``` python
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```

*Batching*ì€ ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ì„ í•œ ë²ˆì— ëª¨ë¸ë¡œ ë³´ë‚´ëŠ” í–‰ë™ì„ ë§í•œë‹¤.
ë§Œì•½ í•œ ê°œì˜ ë¬¸ì¥ì„ ê°€ì§€ê³  ìˆë‹¤ë©´, í•˜ë‚˜ì˜ sequenceë¡œ ë°°ì¹˜ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

``` python
batched_ids = [ids, ids]
```

ì´ê²ƒì€ ë‘ ê°œì˜ ë™ì¼í•œ sequenceì˜ ë°°ì¹˜ì´ë‹¤!

Batchingì€ ëª¨ë¸ì— ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ì„ ì§‘ì–´ë„£ì„ ë•Œ ì‚¬ìš©ëœë‹¤.
ì—¬ëŸ¬ ê°œì˜ sequenceë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ í•˜ë‚˜ì˜ sequenceë¡œ ë°°ì¹˜ë¥¼ ë§Œë“œëŠ” ê²ƒ ë§Œí¼ì´ë‚˜ ê°„ë‹¨í•˜ë‹¤! 
í•˜ì§€ë§Œ, ë‘ ë²ˆì§¸ ë¬¸ì œì ì´ ìˆë‹¤..
ë‘ ê°œ ì´ìƒì˜ ë¬¸ì¥ì„ í•¨ê»˜ ë°°ì¹˜ë¡œ ë§Œë“¤ ë•Œ, ì•„ë§ˆë„ ê·¸ ë¬¸ì¥ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´ë¥¼ ê°€ì§ˆ ê²ƒì´ë‹¤.
ë§Œì•½ ì´ì „ì— tensorë¥¼ ì‚¬ìš©í•´ì„œ ì‘ì—…í•´ë³¸ ì ì´ ìˆë‹¤ë©´, ì´ë“¤ì€ ì •ì‚¬ê°í˜• ëª¨ì–‘ì´ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ê²ƒì´ë‹¤.
ë”°ë¼ì„œ input IDsë¥¼ í•œ ë²ˆì— ë°”ë¡œ tensorë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤..
ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë³´í†µ inputì„ *pad*í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.
