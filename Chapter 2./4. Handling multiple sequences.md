# Handling multiple sequences

이전 절에서는 짧은 한 문장 같은 간단한 예를 살펴보았다.
하지만, 다음과 같은 질문들이 떠오를 것이다.
- 여러 개의 sequence를 어떻게 다뤄야 할까?
- 서로 다른 길이의 여러 개의 sequence를 어떻게 다뤄야 할까?
- vocabulary index는 과연 모델이 가장 잘 작동하도록 허용하는 유일한 입력일까?
- 너무 긴 길이의 sequence에 대해서는 어떻게 해야 할까?

이러한 질문들이 실제로 문제로 나타났을 때, 어떻게 해결해야 할 지 🤗 Transformer API를 이용하여 해결해보자!

### Model은 입력 배치를 예상

이전 예제에서 sequence가 어떻게 숫자 리스트로 변환되는지 살펴보았다. 
이제 이 숫자 리스트를 tensor로 변환해서 모델에 주어보도록 하자!

``` python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```

``` python
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```

오류가 발생했다! 왜 오류가 발생한 걸까? section 2의 pipeline의 step을 따라가보도록 하자.

문제는 모델에 하나의 sequence만이 들어갔기 때문이다.
🤗 Transformer는 기본값으로 여러 개의 문장을 받아들이도록 설정되어 있기 때문에 오류가 발생한 것이다.
여기서 우리는 tokenizer를 sequence에 적용했을 때 백그라운드에서 tokenizer가 수행하는 모든 작업을 수행하려고 했다.
하지만 자세히 들여다보면, tokenizer는 input IDs를 그냥 tensor로 변환하는 것이 아니라, 그 위에 하나의 차원을 더 추가한다.

``` python
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

``` python
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```

새로운 차원을 추가하여 다시 시도해보자!

``` python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])   #ids를 배열 형태로 집어넣음
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```

출력은 다음과 같이 나오게 된다!

``` python
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```

*Batching*은 여러 개의 문장을 한 번에 모델로 보내는 행동을 말한다.
만약 한 개의 문장을 가지고 있다면, 하나의 sequence로 배치를 만들 수 있다.

``` python
batched_ids = [ids, ids]
```

이것은 두 개의 동일한 sequence의 배치이다!

Batching은 모델에 여러 개의 문장을 집어넣을 때 사용된다.
여러 개의 sequence를 사용하는 것은 하나의 sequence로 배치를 만드는 것 만큼이나 간단하다! 
하지만, 두 번째 문제점이 있다..
두 개 이상의 문장을 함께 배치로 만들 때, 아마도 그 문장들은 서로 다른 길이를 가질 것이다.
만약 이전에 tensor를 사용해서 작업해본 적이 있다면, 이들은 정사각형 모양이어야 한다는 것을 알 것이다.
따라서 input IDs를 한 번에 바로 tensor로 변환하는 것은 불가능하다..
이 문제를 해결하기 위해, 보통 input을 *pad*하는 방법을 사용한다.

### 입력 패딩하기

다음의 리스트들은 tensor로 변환될 수 없다.

``` python
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

이 문제를 해결하기 위해, *padding*을 사용하여 tensor가 정사각형 모양을 가질 수 있도록 만들어 줄 것이다.
Padding은 문장에 특별한 토큰인 *padding token*을 추가하여 모든 문장들의 길이를 같게 만들어 준다.
예를 들어, 10개의 단어를 가진 10개의 문장을 가지고 있고, 20개의 단어를 가진 1개의 문장을 가지고 있다고 생각해보자.
이때, padding은 모든 문장을 20개의 단어를 가진 문장으로 변환할 것이다.
우리의 예시에서 결과를 보면, tensor는 다음과 같이 생길 것이다.

``` python
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

padding token ID는 tokenizer.pad_token_id를 통해 찾을 수 있다.
tokenizer.pad_token_id를 사용해서 두 개의 문장을 함께 배치화해서 모델에 주어보도록 하자!

``` python
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```

``` python
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
```

결과를 보게 되면 뭔가 오류가 있다고 생각할 것이다. 
왜냐하면, 두 번째 행의 값이 두 번째 문장의 결괏값과 다르기 때문이다.
분명히 똑같은 문장이지만, 다른 결괏값을 얻게 된 것이다!

이는 Transformer model의 가장 큰 특징 중 하나인 attention layer가 각각의 token을 문맥과 관련짓기 때문이다.
따라서 이러한 특성 때문에, sequence 내의 모든 token을 참조해서, padding token 또한 참조하게 된다.
서로 다른 길이의 문장 각각을 모델에 흘려보내거나, 패딩이 적용된 똑같은 문장을 batch로 해서 흘려보낼 때 똑같은 결과를 얻기 위해서는, attention layer가 padding token을 무시해야할 필요가 있다.
이러한 과정은 attention mask를 사용하면 간단하게 해결된다!

### Attention mask

*Attention mask*는 0과 1로 채워진 input ID tensor와 완벽히 똑같은 모양의 tensor이다.
여기서 1은 참조되어야 하는 토큰을 의미하고, 0은 참조되지 말아야 하는 토큰을 의미한다.

이 attention mask를 이용하여 이전 예시를 완성시켜보자!

``` python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```

``` python
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```

이제야 두 번째 문장에 대해서 똑같은 logit 값을 얻을 수 있게 되었다.
두 번째 문장의 마지막 값은 padding ID이고, 그 값은 attention mask에 의해 0값으로 인식된다는 사실을 이해하고 넘어가도록 하자!

### 더욱 긴 sequence

Transformer model은 model이 받아들일 수 있는 sequence의 최대 길이가 정해져 있다.
대부분의 모델은 512 또는 1024개의 토큰을 다루고, 더욱 긴 길이의 sequence를 처리하려고 하면 충돌이 발생한다.
이러한 문제를 해결하기 위한 방법으로는 두 가지가 있다.

- 더욱 긴 길이의 sequence를 지원하는 모델을 사용
- sequence의 길이 줄이기

model이 지원하는 sequence의 길이는 각각마다 상이하다.
Longformer와 LED는 매우 긴 길이의 sequence를 처리할 수 있는 것으로 유명하다. 
만약 진행해야 하는 작업이 매우 긴 길의 sequence를 처리해야 한다면 이런 모델들을 사용해보는 것을 추천한다.

다른 방법으로는, max_sequence_length 파라미터를 명시해서 sequence의 길이를 줄이기를 추천한다!

``` python
sequence = sequence[:max_sequence_length]
```
