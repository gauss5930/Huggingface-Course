# Handling multiple sequences

이전 절에서는 짧은 한 문장 같은 간단한 예를 살펴보았다.
하지만, 다음과 같은 질문들이 떠오를 것이다.
- 여러 개의 sequence를 어떻게 다뤄야 할까?
- 서로 다른 길이의 여러 개의 sequence를 어떻게 다뤄야 할까?
- vocabulary index는 과연 모델이 가장 잘 작동하도록 허용하는 유일한 입력일까?
- 너무 긴 길이의 sequence에 대해서는 어떻게 해야 할까?

이러한 질문들이 실제로 문제로 나타났을 때, 어떻게 해결해야 할 지 🤗 Transformer API를 이용하여 해결해보자!

### Model은 입력 배치를 예상

이전 예제에서 sequence가 어떻게 숫자 리스트로 변환되는지 살펴보았다. 
이제 이 숫자 리스트를 tensor로 변환해서 모델에 주어보도록 하자!

``` python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```

``` python
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
```

오류가 발생했다! 왜 오류가 발생한 걸까? section 2의 pipeline의 step을 따라가보도록 하자.

문제는 모델에 하나의 sequence만이 들어갔기 때문이다.
🤗 Transformer는 기본값으로 여러 개의 문장을 받아들이도록 설정되어 있기 때문에 오류가 발생한 것이다.
여기서 우리는 tokenizer를 sequence에 적용했을 때 백그라운드에서 tokenizer가 수행하는 모든 작업을 수행하려고 했다.
하지만 자세히 들여다보면, tokenizer는 input IDs를 그냥 tensor로 변환하는 것이 아니라, 그 위에 하나의 차원을 더 추가한다.

``` python
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```

``` python
tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])
```

새로운 차원을 추가하여 다시 시도해보자!

``` python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])   #ids를 배열 형태로 집어넣음
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```

출력은 다음과 같이 나오게 된다!

``` python
Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]
```

*Batching*은 여러 개의 문장을 한 번에 모델로 보내는 행동을 말한다.
만약 한 개의 문장을 가지고 있다면, 하나의 sequence로 배치를 만들 수 있다.

``` python
batched_ids = [ids, ids]
```

이것은 두 개의 동일한 sequence의 배치이다!

Batching은 모델에 여러 개의 문장을 집어넣을 때 사용된다.
여러 개의 sequence를 사용하는 것은 하나의 sequence로 배치를 만드는 것 만큼이나 간단하다! 
하지만, 두 번째 문제점이 있다..
두 개 이상의 문장을 함께 배치로 만들 때, 아마도 그 문장들은 서로 다른 길이를 가질 것이다.
만약 이전에 tensor를 사용해서 작업해본 적이 있다면, 이들은 정사각형 모양이어야 한다는 것을 알 것이다.
따라서 input IDs를 한 번에 바로 tensor로 변환하는 것은 불가능하다..
이 문제를 해결하기 위해, 보통 input을 *pad*하는 방법을 사용한다.
