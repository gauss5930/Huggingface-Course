# Putting it all together

앞의 section들에서, 하나하나 직접 해보는 과정을 통해 자세하게 이해하고 넘어왔다.
어떻게 tokenizer가 작동하는지와 토큰화, input ID 변환, padding, truncation, attention mask에 대해서 배웠다.

하지만, section 2에서 봐서 알 수 있듯이, 🤗 Transformer API는 이 모든 것을 한 번에 다룰 수 있는 high-level의 함수를 가지고 있다.
tokenizer을 문장에 사용하여 바로 모델에 집어넣을 수 있는 입력을 만들 수 있는 것처럼 말이다!

``` python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

여기서 model_input 변수는 model이 잘 작동할 수 있는데 필요한 모든 것들을 포함하고 있다.
DistilBERT의 경우에는 input ID 뿐만 아니라 attention mask도 포함하고 있다. 
추가 입력을 허용하는 다른 모델들도 tokenizer 객체에 의한 출력을 가진다.

아래에서 보게 될 예시처럼, 이 방식은 매우 강력하다.
첫 번째로, 하나의 sequence에 대해 토큰화를 하는 방법을 보자.

``` python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

또한, tokenizer는 API의 변화 없이 여러 개의 sequence도 한 번에 처리가 가능하다.

``` python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

목표에 따라서, padding 또한 가능하다.

``` python
# sequence의 최대 길이 맞춰서 padding
model_inputs = tokenizer(sequences, padding="longest")

# model의 max_length에 맞춰서 padding
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# 지정된 길이에 맞춰서 padding
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

sequence의 길이를 줄이는 것 또한 가능하다.

``` python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# model의 max_length 보다 더 긴 문장을 truncate
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# 지정된 max_length에 알맞게 truncate
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

tokenizer는 모델에 바로 적용될 수 있는 특정한 tensor 프레임워크로 바로 변환할 수 있다.
예를 들어, 다음의 코드에서 tokenizer로 서로 다른 프레임워크의 tensor를 출력하는 모습을 확인할 수 있다.
- 'pt': Pytorch tensor
- 'tf': TensorFlow tensor
- 'np': NumPy array

``` python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# PyTorch tensors 반환
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# TensorFlow tensors 반환
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# NumPy arrays 반환
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

### 스페셜 토큰

tokenizer을 통해 만들어진 input ID를 살펴보면, 이전에 얻었던 값과 살짝 다르다는 것을 알 수 있다.

``` python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

``` python
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

처음의 출력을 보면 맨 앞과, 맨 뒤에 하나의 token ID가 있는 것을 알 수 있다. 이 두 개의 sequence를 decode해서 알아보도록 하자.

``` python
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

``` python
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

결과를 보니, tokenizer가 시작 부분에 [CLS]와 마지막 부분에 [SEP]라는 스페셜 토큰을 추가한 것을 알 수 있다.
이는 왜냐하면, 모델이 pre-train될 때, 이 스페셜 토큰을 사용하여 pre-train 되었기 때문에, 추론을 할 때도 이 값이 추가되어야만 하기 때문이다.
아, 그렇다고 모든 모델이 다 그렇다는 것은 아니고, 다른 토큰을 추가하기도 한다.
모델은 이 스페셜 토큰을 항상 앞이나 뒤에만 추가한다.
어쨌든 tokenizer는 어떤 것이 예상되는지 알고 이를 처리할 것이다.

### 요약: tokenizer부터 model까지

자, 이렇게 해서 tokenizer가 text에 적용될 때, 어떠한 과정을 거치는지 각각 살펴보았다.
마지막으로 tokenizer가 padding을 사용하여 어떻게 여러 개의 sequence를 다루는지와 길이가 긴 sequence에 대해서 truncation을 이용하여 어떻게 sequence의 길이를 조정하는지 알아보도록 하자!

``` python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
