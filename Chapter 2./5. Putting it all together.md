# Putting it all together

ì•ì˜ sectionë“¤ì—ì„œ, í•˜ë‚˜í•˜ë‚˜ ì§ì ‘ í•´ë³´ëŠ” ê³¼ì •ì„ í†µí•´ ìì„¸í•˜ê²Œ ì´í•´í•˜ê³  ë„˜ì–´ì™”ë‹¤.
ì–´ë–»ê²Œ tokenizerê°€ ì‘ë™í•˜ëŠ”ì§€ì™€ í† í°í™”, input ID ë³€í™˜, padding, truncation, attention maskì— ëŒ€í•´ì„œ ë°°ì› ë‹¤.

í•˜ì§€ë§Œ, section 2ì—ì„œ ë´ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´, ğŸ¤— Transformer APIëŠ” ì´ ëª¨ë“  ê²ƒì„ í•œ ë²ˆì— ë‹¤ë£° ìˆ˜ ìˆëŠ” high-levelì˜ í•¨ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆë‹¤.
tokenizerì„ ë¬¸ì¥ì— ì‚¬ìš©í•˜ì—¬ ë°”ë¡œ ëª¨ë¸ì— ì§‘ì–´ë„£ì„ ìˆ˜ ìˆëŠ” ì…ë ¥ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê²ƒì²˜ëŸ¼ ë§ì´ë‹¤!

``` python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

ì—¬ê¸°ì„œ model_input ë³€ìˆ˜ëŠ” modelì´ ì˜ ì‘ë™í•  ìˆ˜ ìˆëŠ”ë° í•„ìš”í•œ ëª¨ë“  ê²ƒë“¤ì„ í¬í•¨í•˜ê³  ìˆë‹¤.
DistilBERTì˜ ê²½ìš°ì—ëŠ” input ID ë¿ë§Œ ì•„ë‹ˆë¼ attention maskë„ í¬í•¨í•˜ê³  ìˆë‹¤. 
ì¶”ê°€ ì…ë ¥ì„ í—ˆìš©í•˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸ë“¤ë„ tokenizer ê°ì²´ì— ì˜í•œ ì¶œë ¥ì„ ê°€ì§„ë‹¤.

ì•„ë˜ì—ì„œ ë³´ê²Œ ë  ì˜ˆì‹œì²˜ëŸ¼, ì´ ë°©ì‹ì€ ë§¤ìš° ê°•ë ¥í•˜ë‹¤.
ì²« ë²ˆì§¸ë¡œ, í•˜ë‚˜ì˜ sequenceì— ëŒ€í•´ í† í°í™”ë¥¼ í•˜ëŠ” ë°©ë²•ì„ ë³´ì.

``` python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

ë˜í•œ, tokenizerëŠ” APIì˜ ë³€í™” ì—†ì´ ì—¬ëŸ¬ ê°œì˜ sequenceë„ í•œ ë²ˆì— ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤.

``` python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

ëª©í‘œì— ë”°ë¼ì„œ, padding ë˜í•œ ê°€ëŠ¥í•˜ë‹¤.

``` python
# sequenceì˜ ìµœëŒ€ ê¸¸ì´ ë§ì¶°ì„œ padding
model_inputs = tokenizer(sequences, padding="longest")

# modelì˜ max_lengthì— ë§ì¶°ì„œ padding
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# ì§€ì •ëœ ê¸¸ì´ì— ë§ì¶°ì„œ padding
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

sequenceì˜ ê¸¸ì´ë¥¼ ì¤„ì´ëŠ” ê²ƒ ë˜í•œ ê°€ëŠ¥í•˜ë‹¤.

``` python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# modelì˜ max_length ë³´ë‹¤ ë” ê¸´ ë¬¸ì¥ì„ truncate
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# ì§€ì •ëœ max_lengthì— ì•Œë§ê²Œ truncate
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

tokenizerëŠ” ëª¨ë¸ì— ë°”ë¡œ ì ìš©ë  ìˆ˜ ìˆëŠ” íŠ¹ì •í•œ tensor í”„ë ˆì„ì›Œí¬ë¡œ ë°”ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒì˜ ì½”ë“œì—ì„œ tokenizerë¡œ ì„œë¡œ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ì˜ tensorë¥¼ ì¶œë ¥í•˜ëŠ” ëª¨ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
- 'pt': Pytorch tensor
- 'tf': TensorFlow tensor
- 'np': NumPy array

``` python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# PyTorch tensors ë°˜í™˜
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# TensorFlow tensors ë°˜í™˜
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# NumPy arrays ë°˜í™˜
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

### ìŠ¤í˜ì…œ í† í°

tokenizerì„ í†µí•´ ë§Œë“¤ì–´ì§„ input IDë¥¼ ì‚´í´ë³´ë©´, ì´ì „ì— ì–»ì—ˆë˜ ê°’ê³¼ ì‚´ì§ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

``` python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

``` python
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

ì²˜ìŒì˜ ì¶œë ¥ì„ ë³´ë©´ ë§¨ ì•ê³¼, ë§¨ ë’¤ì— í•˜ë‚˜ì˜ token IDê°€ ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ ë‘ ê°œì˜ sequenceë¥¼ decodeí•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ì.

``` python
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

``` python
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

ê²°ê³¼ë¥¼ ë³´ë‹ˆ, tokenizerê°€ ì‹œì‘ ë¶€ë¶„ì— [CLS]ì™€ ë§ˆì§€ë§‰ ë¶€ë¶„ì— [SEP]ë¼ëŠ” ìŠ¤í˜ì…œ í† í°ì„ ì¶”ê°€í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
ì´ëŠ” ì™œëƒí•˜ë©´, ëª¨ë¸ì´ pre-trainë  ë•Œ, ì´ ìŠ¤í˜ì…œ í† í°ì„ ì‚¬ìš©í•˜ì—¬ pre-train ë˜ì—ˆê¸° ë•Œë¬¸ì—, ì¶”ë¡ ì„ í•  ë•Œë„ ì´ ê°’ì´ ì¶”ê°€ë˜ì–´ì•¼ë§Œ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
ì•„, ê·¸ë ‡ë‹¤ê³  ëª¨ë“  ëª¨ë¸ì´ ë‹¤ ê·¸ë ‡ë‹¤ëŠ” ê²ƒì€ ì•„ë‹ˆê³ , ë‹¤ë¥¸ í† í°ì„ ì¶”ê°€í•˜ê¸°ë„ í•œë‹¤.
ëª¨ë¸ì€ ì´ ìŠ¤í˜ì…œ í† í°ì„ í•­ìƒ ì•ì´ë‚˜ ë’¤ì—ë§Œ ì¶”ê°€í•œë‹¤.
ì–´ì¨Œë“  tokenizerëŠ” ì–´ë–¤ ê²ƒì´ ì˜ˆìƒë˜ëŠ”ì§€ ì•Œê³  ì´ë¥¼ ì²˜ë¦¬í•  ê²ƒì´ë‹¤.

### ìš”ì•½: tokenizerë¶€í„° modelê¹Œì§€

ì, ì´ë ‡ê²Œ í•´ì„œ tokenizerê°€ textì— ì ìš©ë  ë•Œ, ì–´ë– í•œ ê³¼ì •ì„ ê±°ì¹˜ëŠ”ì§€ ê°ê° ì‚´í´ë³´ì•˜ë‹¤.
ë§ˆì§€ë§‰ìœ¼ë¡œ tokenizerê°€ paddingì„ ì‚¬ìš©í•˜ì—¬ ì–´ë–»ê²Œ ì—¬ëŸ¬ ê°œì˜ sequenceë¥¼ ë‹¤ë£¨ëŠ”ì§€ì™€ ê¸¸ì´ê°€ ê¸´ sequenceì— ëŒ€í•´ì„œ truncationì„ ì´ìš©í•˜ì—¬ ì–´ë–»ê²Œ sequenceì˜ ê¸¸ì´ë¥¼ ì¡°ì •í•˜ëŠ”ì§€ ì•Œì•„ë³´ë„ë¡ í•˜ì!

``` python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```
