# Tokenizers

Tokenizer는 NLP pipeline에서 가장 중요한 요소 중 하나이다.
Tokenizer는 오직 하나의 목표를 가지고 있는데, text를 model이 처리할 수 있는 데이터로 변환하는 것이다.
Model은 숫자만을 처리할 수 있기 때문에, tokenizer는 text를 숫자 데이터로 바꿔야할 필요가 있다.
이번 절에서는 tokenization pipeline에서 어떤 일이 일어나는지 자세하게 알아보도록 하자!

NLP task에서 데이터는 일반적으로 생 텍스트로 처리되는 경우가 많다. 다음의 텍스트 예시를 봐보도록 하자.

```
Jim Henson was a puppeteer
```

하지만, 모델은 오직 숫자만을 처리할 수 있기 때문에, 이 생 텍스트를 숫자로 변환할 방법을 찾아야 한다.
이것이 바로 tokenizer가 하는 일이고, 이를 하는 방법에는 여러 가지가 있다.
목표는 가장 의미있는 representation을 찾는 것이다. 그리고 가능하다면 가장 작은 representation을 찾는 것이다!

tokenizer 알고리즘의 몇 가지 예시를 둘러보고, tokenization에 대한 몇 가지 질문에 응답해보도록 하자.

### Word-based

첫 번째 tokenizer 유형은 '단어 기반(*word-base*)'이다.
이는 일반적으로 가장 쉽게 설정할 수 있고 적은 규칙으로 사용할 수 있고, 준수한 결과를 내놓는다.
예를 들어, 아래의 이미지를 보라. 목표는 생 텍스트를 word 단위로 나누고, 각각에 대한 숫자 representation을 찾는 것이다.

![word_based_tokenization](https://user-images.githubusercontent.com/80087878/218662904-9b3c65a4-b2d3-4abc-bbf3-d7967121f675.svg)

text를 나누는 데에는 여러 가지 방법들이 있다. 예를 들어, Python의 split() 함수를 사용하여 공백에 대해 text를 word단위로 tokenize 해보자.

``` python
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
```

``` python
['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

이러한 방식의 tokenizer을 사용하면, 매우 큰 규모의 'vocabulary'를 얻을 수 있는데, 이는 corpus로부터 얻게 된 각각의 독립적인 토큰들이 정의되어 있다.
각각의 단어들은 지정된 index를 갖고, 0부터 시작해서 점점 규모가 커져가는 구조이다.
vocabulary는 이 index를 사용하여 각각의 단어를 식별한다.

만약, 한 언어에 대해 이 word-base tokenizer로 완벽하게 커버하고 싶다면, 이 언어 내에서 각각의 단어를 식별하는 식별자가 필요하다.
이 식별자는 엄청난 양의 token을 생성할 것이다.
예를 들어, English에 500,000개의 단어가 있다고 해보자. 각 단어에서 입력 ID로의 맵을 작성하려면 많은 ID를 추적해야 한다.
게다가, '강아지' 같은 단어들은 '강아지들'과 다르게 표현되고, 모델은 초기에 이 '강아지'와 '강아지들'이 얼마나 유사한지 알 수 있는 방법이 없다.
따라서, 모델은 두 개의 단어가 서로 연관되어 있지 않다고 식별할 것이다.

마지막으로, vocabulary에 포함되어 있지 않은 토큰을 표현할 수 있는 커스텀 토큰도 필요하다. 
주로 '[UNK]'라고 표현되는 'unknown' 토큰이다.
이 토큰이 존재한다는 것은 좋지 않은 신호로, 만약 이 토큰이 너무 많다면 text로부터 유의미한 결과를 도출해내지 못하는 경우에까지 이를 수가 있다.
그래서 vocabulary를 제작할 때 가장 중요한 목표 중 하나는 이 unknown token이 최대한 적게 나올 수 있도록 만드는 것이다.

unknown token을 줄이는 방법 중 하나는 더욱 깊이 들어가는 것인데, 여기에 사용되는 방법이 다음으로 살펴볼 tokenizer인 *character-based tokenizer*이다.

### Character-based

Character-based tokenizer은 텍스트를 word 단위가 아닌 character 단위로 자른다. 이렇게 하면 다음의 두 가지 장점을 얻을 수 있다.
- vocabulary의 크기가 작아짐.
- unknow token의 수가 줄어듦. 왜냐하면 모든 단어가 character 단위로 만들어질 수 있기 떄문.

하지만, 공백과 구두법에 관한 의문이 생기기 시작할 것이다.

![character_based_tokenization](https://user-images.githubusercontent.com/80087878/218666493-bcb9928e-d229-4104-bccc-0c2adcad174b.svg)

이 방식도 완벽하지는 않다. 왜냐하면, word가 아니라 character 단위로 representation을 만드는 것은 직관적으로, 의미를 더욱 적게 함유하고 있기 때문이다.
각각의 character는 그 자신으로는 큰 의미를 가지지 못하지만, word는 그 자신으로 의미를 가지기 때문이다.
하지만, 이는 언어에 따라서 다르다는 점을 알아두길 바란다. (ex. 중국어)

또 고려해야할 점은 우리의 모델이 엄청나게 많은 양의 토큰을 처리해야 한다는 것이다. 
word-based tokenizer는 하나의 word를 하나의 token으로 인식하지만, 이를 character로 변환하면 10개 또는 그 이상의 토큰이 될 수 있다.

이 두 개의 방식 중 더 나은 방법을 선택하고자 나온 것이, 이 두 방식을 합친 *subword tokenization*이다.

### Subword tokenization

Subword tokenization 알고리즘은 자주 사용되는 단어들은 더욱 작은 subword로 나눠지지 않지만, 드물게 사용되는 단어들은 의미를 가지는 subword로 나뉘어질 것이라는 원리에 기반을 두고 있다.
예를 들어, 'annoyingly'는 드물게 사용되는 단어로 인식되고, 'annoying'과 'ly'로 분해될 것이다.
이 둘은 독립 subword로 더 자주 나타날 가능성이 높으며 동시에 "annoyingly"의 의미는 "annoying"와 "ly"의 합성 의미로 유지된다.

subword tokenization 알고리즘이 어떻게 sequence "Let's do tokenization!"을 토큰화하는지 살펴보도록 하자.

![bpe_subword](https://user-images.githubusercontent.com/80087878/218668473-828c2ace-1018-4fa5-a1d4-aec82cfc5a05.svg)

이러한 subword는 수많은 semantic 의미를 가지고 있다.
예를 들어, 위의 예시에서 'tokenization'은 'token'과 'ization'으로 나뉘게 된다. 
두 개의 토큰은 각각 구문적 의미를 갖지만 더욱 공간적으로 효율적이다.
이러한 점은 더욱 작은 크기의 vocabulary를 갖게 해주고, unknown 토큰을 거의 없게 만들어 준다.

### Loading & Saving

model을 불러오고 저장하는 것만큼이나, tokenizer을 불러오고 저장하는 것은 쉽다!
사실 이 둘은 똑같은 방식에 기반을 두고 있어서, 똑같이 from_pretrained()와 save_pretrained()를 사용한다.
이 method들은 tokenizer에 의해 사용되는 알고리즘 뿐만 아니라 vocabulary도 불러오고 저장해준다.

BERT tokenizer을 불러오는 것은 model을 불러오는 것과 똑같은 방식으로 불러온다. 다음의 코드를 살펴보자.

``` python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```

AutoModel과 유사하게, AutoTokenizer 클래스는 체크포인트 이름에 알맞는 tokenizer class를 가져다 준다.

``` python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

이제 이전 절에서 사용했던 것처럼 Tokenizer을 사용할 수 있다.

``` python
tokenizer("Using a Transformer network is simple")
```

``` python
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

Tokenizer을 저장하는 것은 model을 저장하는 것과 동일하다.

``` python
tokenizer.save_pretrained("directory_on_my_computer")
```

token_type_ids에 대해서는 Chapter 3.에서 이야기 해보도록 하고, attention_mask key에 대해서도 곧 얘기할 것이다.
자 그러면 다시 돌아와서, 어떻게 input_ids가 생성되는지 살펴보도록 하자. 이를 위해서, tokenizer의 중간 method를 살펴볼 필요가 있다.

### Encoding

text를 숫자로 변환하는 것은 *encoding*이라고 알려져 있다. Encoding은 두 단계로 진행되는데, tokenization과 input IDs를 변환하는 것이다.

앞서 봤듯이, 첫 번째 단계는 text를 word 단위로 쪼갠 것을 *token*이라 부른다.
이를 하는 방법에는 여러 가지가 있는데, 모델이 사전 학습될 때 사용된 tokenizer와 똑같은 tokenizer을 사용할 수 있도록, 모델의 이름을 사용하여 정의하여야 한다.

두 번째 단계는, 이 토큰들을 숫자로 변환하는 것이다. 이를 통해 tensor을 생성하여 model에 사용할 수 있다.
이를 위해, tokenizer은 from_pretrained() 메서드로 인스턴스화할 때 다운로드한 vocabulary을 가진다.
다시 한 번 말하지만, 모델이 사전 학습될 때 사용된 vocabulary와 똑같은 vocabulary가 사용되어야 한다.

이 두 단계에 대해 더욱 잘 이해하기 위해, 각각에 대해 따로따로 알아볼 것이다.
미리 공지하자면, tokenization pipeline을 따로따로 사용하여 중간 과정에 대한 결과를 보여주지만, 실전에서는 그럴 필요 없이 바로 tokenizer을 불러와서 사용하면 된다.

#### Tokenization

토큰화 과정은 tokenizer의 tokenize() method에 의해 진행된다.

``` python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
```

이 method의 출력은 문자열 또는 토큰의 리스트이다.

``` python
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

이 tokenizer은 subword tokenizer로, vocabulary로 표현될 수 있는 단어를 얻을 때까지 단어를 분할한다.
예를 들어, transformer라는 단어는 'transform'과 '##er'의 두 개의 토큰으로 나뉘어진다.

#### From tokens to input IDs

입력 ID로의 변환은 convert_tokens_to_ids() method에 의해 처리된다.

``` python
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
```

``` python
[7993, 170, 11303, 1200, 2443, 1110, 3014]
```

적합한 tensor 프레임워크로 변환된 출력은 이 Chapter의 앞에서 봤듯이 모델의 입력으로 사용될 수 있다.

### Decoding

*Decoding*은 반대되는 방향으로 진행된다.
vocabulary index들에서 원하는 문자를 가져오는 느낌이다.
이것도 매우 간단한데, decode() method를 사용하면 된다!

``` python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```

``` python
'Using a Transformer network is simple'
```

decode() method는 인덱스를 다시 토큰으로 변환할 뿐만 아니라, 동일한 단어의 일부인 토큰을 함께 그룹화하여 읽을 수 있는 문장을 생성한다.
이러한 특성은 새로운 텍스트를 예측할 때 매우 유용하게 사용될 수 있다.
