# Slice & Dice

ì•ìœ¼ë¡œë„ ë§ì€ taskë¥¼ ì§„í–‰í• í…ë°, ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì—ëŠ” ì´ ë°ì´í„°ê°€ modelì— ì í•©í•˜ì§€ ì•Šì€ í˜•íƒœì¼ ê²ƒì´ë‹¤.
ê·¸ë˜ì„œ ì´ë²ˆ ì„¹ì…˜ì—ì„œëŠ” ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì œê³µí•˜ëŠ” dataset cleaning featureë“¤ì„ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.

### Slicing & Dicing data

Pandasì™€ ìœ ì‚¬í•˜ê²Œ, ğŸ¤— DatasetsëŠ” Datasetê³¼ DatasetDict objectì— ëŒ€í•´ì„œ ì¡°ì¢…í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ í•¨ìˆ˜ë¥¼ ì œê³µí•œë‹¤.
ì´ë¯¸, Chapter 3. ì—ì„œ ìš°ë¦¬ëŠ” Dataset.map() methodë¥¼ ê²½í—˜í•˜ì˜€ë‹¤.
ë”°ë¼ì„œ ì´ë²ˆ ì„¹ì…˜ì—ì„œëŠ” ì´ ì™¸ì— ë”ìš± ë‹¤ì–‘í•œ í•¨ìˆ˜ë“¤ì— ëŒ€í•´ ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤! ğŸ”¥

ì˜ˆì‹œë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì‚¬ìš©í•  ë°ì´í„°ì…‹ì€ [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29)ìœ¼ë¡œ ì§„í–‰í•˜ê² ë‹¤.
ì´ ë°ì´í„°ì…‹ì€ ë‹¤ì–‘í•œ ì•½í’ˆì— ëŒ€í•œ í™˜ìë“¤ì˜ ë¦¬ë·°ì™€ ì•½ì„ ë³µìš©í–ˆì„ ë•Œ ì»¨ë””ì…˜ì˜ ë³€í™”ë¥¼ 10ì  ë§Œì ìœ¼ë¡œ í•´ì„œ í‰ê°€í•˜ê³  ìˆë‹¤.

ê°€ì¥ ì²˜ìŒìœ¼ë¡œ í•´ì•¼í•  ê²ƒì€ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì¶”ì¶œí•˜ëŠ” ê²ƒì´ë‹¤.
ì´ëŠ” wgetê³¼ unzip ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ì„œ í•  ìˆ˜ ìˆë‹¤.

``` python
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
!unzip drugsCom_raw.zip
```

TSVëŠ” CSVì˜ ë³€í˜•ìœ¼ë¡œ tabì„ separatorë¡œ ë°›ëŠ”ë‹¤.
ì´ëŸ¬í•œ í˜•ì‹ì˜ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ë•ŒëŠ”, csvë¥¼ ë¶ˆëŸ¬ì˜¬ ë•Œë‘ ë˜‘ê°™ì€ë° load_dataset()ì˜ delimiter ì¸ìë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ëª…ì‹œí•´ì„œ ì‚¬ìš©í•˜ë©´ ëœë‹¤.

``` python
from datasets import load_dataset

data_files = {"train": "drugsComTrain_raw.tsv", "test": "drugsComTest_raw.tsv"}
# \tëŠ” Pythonì—ì„œ tabì„ ì˜ë¯¸í•œë‹¤.
drug_dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
```

ì–´ë– í•œ ì¢…ë¥˜ì˜ ë°ì´í„°ë¡œ ì‘ì—…ì„ í•˜ë“ , ê·¸ ë°ì´í„°ì˜ ìœ í˜•ì— ëŒ€í•´ ë¹ ë¥´ê²Œ í™•ì¸í•˜ëŠ” ë°©ë²•ì€ ëœë¤í•˜ê²Œ ì†ŒëŸ‰ì˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ì„œ í™•ì¸í•´ë³´ëŠ” ê²ƒì´ë‹¤.
ğŸ¤— Datasetsì—ì„œëŠ”, Dataset.shuffle()ê³¼ Dataset.select() í•¨ìˆ˜ë¥¼ ì—°ê²°í•´ì„œ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•  ìˆ˜ ìˆë‹¤.

``` python
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
# ì²˜ìŒ ëª‡ ê°œì˜ ì˜ˆì‹œ ì‚´í´ë³´ê¸°
drug_sample[:3]
```

``` python
{'Unnamed: 0': [87571, 178045, 80482],
 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],
 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],
 'review': ['"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"',
  '"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."',
  '"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."'],
 'rating': [9.0, 3.0, 10.0],
 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],
 'usefulCount': [36, 13, 128]}
```

ì¬í˜„ì„±ì„ ìœ„í•´ì„œ Dataset.shuffle()ì˜ seedë¥¼ ìˆ˜ì •í•˜ì˜€ë‹¤.
Dataset.select()ëŠ” ë°˜ë³µì ì¸ indexë¥¼ ì˜ˆìƒí•˜ë¯€ë¡œ range(1000)ì„ ì „ë‹¬í•˜ì—¬ ì…”í”Œëœ ë°ì´í„°ì…‹ì—ì„œ ì²˜ìŒ 1,000ê°œì˜ ì˜ˆì œë¥¼ ê°€ì ¸ì™”ë‹¤.
ì´ ë°ì´í„°ì…‹ì—ì„œ ë‚˜ì˜¨ ìƒ˜í”Œì—ì„œ ìš°ë¦¬ëŠ” ì´ë¯¸ ëª‡ ê°œì˜ ì´ìƒí•œ ì ì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.

- Unnamed: 0 ì—´ì€ ê° í™˜ìì— ëŒ€í•´ ìµëª…í™”ëœ IDì²˜ëŸ¼ ì˜ì‹¬ìŠ¤ëŸ½ê²Œ ë³´ì¸ë‹¤.
- condition ì—´ì€ ëŒ€ë¬¸ìì™€ ì†Œë¬¸ì ë¼ë²¨ì„ ì´ˆí•¨í•˜ê³  ìˆë‹¤.
- ë¦¬ë·°ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ ë‹¤ì–‘í•œ ê¸¸ì´ì™€ Pythonì˜ line separator(\r \n) ë¿ë§Œ ì•„ë‹ˆë¼ HTMLì˜ character code(&\#039)ì˜ ì¡°í•©ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.

ìœ„ì—ì„œ ë‚˜ì—´í•œ ê°ê°ì˜ ì´ìŠˆë“¤ì— ëŒ€í•´ì„œ ì–´ë–»ê²Œ ğŸ¤— Datasetsë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€ ì•Œì•„ë³´ì.
Unnamed: 0 ì—´ì— ëŒ€í•œ í™˜ì ID ê°€ì„¤ì„ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´, Dataset.unique() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ID ìˆ˜ê°€ ê° splitì˜ í–‰ ìˆ˜ì™€ ì¼ì¹˜í•˜ëŠ” ì§€ í™•ì¸í•˜ë©´ ëœë‹¤.

``` python
for split in drug_dataset.keys():
    assert len(drug_dataset[split]) == len(drug_dataset[split].unique("Unnamed: 0"))
```

ì´ë ‡ê²Œ í•˜ë©´ ì´ ê°€ì„¤ì„ í™•ì¸í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤.
ê·¸ë ‡ë‹¤ë©´ Unnamed: 0 ì—´ì„ ì¢€ ë” í•´ì„ ê°€ëŠ¥í•œ ì´ë¦„ìœ¼ë¡œ ë³€ê²½í•´ì„œ, ë°ì´í„°ì…‹ì„ ì¢€ ë” ì •ë¦¬í•´ë³´ì.
DatasetDict.rename_column() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ë‘ splitì˜ ì—´ì˜ ì´ë¦„ì„ í•œ ë²ˆì— ë³€ê²½í•  ìˆ˜ ìˆë‹¤.

``` python
drug_dataset = drug_dataset.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
drug_dataset
```

``` python
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 161297
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 53766
    })
})
```

ë‹¤ìŒìœ¼ë¡œ, Dataset.map()ì„ ì‚¬ìš©í•´ì„œ condition ë¼ë²¨ì„ ì •ê·œí™”í•´ë³´ì.
Chapter 3. ì—ì„œ tokenizationìœ¼ë¡œ í–ˆë˜ ê²ƒì²˜ëŸ¼, drug_datasetì˜ ê° splitì˜ ëª¨ë“  í–‰ì— ëŒ€í•´ ì ìš©ë  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ì •ì˜í•´ë³´ì.

``` python
def lowercase_condition(example):
    return {"condition": example["condtion"].lower()}
    
drug_dataset.map(lowercase_condition)
```

``` python
AttributeError: 'NoneType' object has no attribute 'lower'
```

í•˜ì§€ë§Œ map í•¨ìˆ˜ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.. ğŸ˜¢
ì´ ì˜¤ë¥˜ë¡œë¶€í„° condition ì—´ì— ë¬¸ìì—´ë¡œ ì†Œë¬¸ì ë³€í™˜ì´ ë¶ˆê°€ëŠ¥í•œ None ê°’ì´ ìˆë‹¤ëŠ” ê²ƒì„ ì¶”ë¡ í•  ìˆ˜ ìˆì—ˆë‹¤.
Dataset.mep()ê³¼ ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ ì‘ë™í•˜ê³ , ë°ì´í„°ì…‹ì˜ í•œ ê°€ì§€ ì˜ˆì‹œë¥¼ ë°›ëŠ” í•¨ìˆ˜ì¸ Dataset.filter()ì„ ì‚¬ìš©í•´ì„œ Noneê°’ì„ ê°€ì§€ëŠ” í–‰ì„ ì‚­ì œí•´ë³´ì.

``` python
def filter_noise(x):
    return x["condition"] is not None
```

ê·¸ ë‹¤ìŒì—, drug_dataset.filter(filter_nones)ë¥¼ *lambda* í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ í•œ ì¤„ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆë‹¤.
Pythonì—ì„œëŠ”, lambda í•¨ìˆ˜ë¥¼ ë”±íˆ ëª…ëª…í•˜ì§€ ì•Šê³  ì‘ì€ í•¨ìˆ˜ì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•œë‹¤.

``` python
lambda <arguments> : <expression>
```

ì—¬ê¸°ì„œ lambdaëŠ” Pythonì˜ íŠ¹ë³„í•œ í‚¤ì›Œë“œì´ê³ , <arguments>ëŠ” ì‰¼í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ ì§€ëŠ” ê°’ìœ¼ë¡œ í•¨ìˆ˜ì— ëŒ€í•œ ì…ë ¥ì„ ì •ì˜í•˜ê³ , <expression>ì€ ì‹¤í–‰í•  ì—°ì‚°ì„ í‘œí˜„í•œë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ìˆ«ìë¥¼ ì œê³±í•˜ëŠ” ê°„ë‹¨í•œ ì—°ì‚°ì„ lambdaë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
  
``` python
lambda x : x * x
```

ì´ í•¨ìˆ˜ë¥¼ ì…ë ¥ì— ì ìš©í•˜ê¸° ìœ„í•´, ì´ í•¨ìˆ˜ì™€ ì…ë ¥ì„ ê´„í˜¸ë¡œ ê°ì‹¸ì•¼ í•œë‹¤.

``` python
(lambda x: x * x)(3)
```

``` python
9
```

ì´ì™€ ë¹„ìŠ·í•˜ê²Œ, ì‰¼í‘œë¡œ ë‚˜ëˆ„ì–´ì„œ lambda í•¨ìˆ˜ë¥¼ ì—¬ëŸ¬ ê°œì˜ ì¸ìë¡œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ì‚¼ê°í˜•ì˜ ë„“ì´ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì„ ë‹¤ìŒê³¼ ê°™ì€ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

``` python
(lambda base, height: 0.5 * base * height)(4, 8)
```

``` python
16.0
```

lambda í•¨ìˆ˜ëŠ” ì‘ê³ , í•œ ë²ˆë§Œ ì‚¬ìš©í•  ì°¸ìˆ˜ë¥¼ ì •ì˜í•  ë•Œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ëœë‹¤.
ğŸ¤— Datasets ë¬¸ë§¥ì—ì„œëŠ”, lambda í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ê°„ë‹¨í•œ mapê³¼ filter ì—°ì‚°ì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
ê·¸ë˜ì„œ, ì´ íŠ¸ë¦­ì„ ì‚¬ìš©í•´ì„œ ë°ì´í„°ì…‹ì—ì„œ Noneê°’ì„ ì œê±°í•  ìˆ˜ ìˆë‹¤.

``` python
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None)
```

Noneê°’ì´ ì œê±°ë¨ê³¼ í•¨ê»˜, condition ì—´ì„ ì •ê·œí™”í•  ìˆ˜ ìˆë‹¤.

``` python
drug_dataset = drug_dataset.map(lowercase_condition)
# ì†Œë¬¸ìí™”ê°€ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸°
drug_dataset["train"]["condition"][:3]
```

``` python
['left venticular dysfunction', 'adhd', 'birth control']
```

ì˜ ì‘ë™ë˜ì—ˆë‹¤! ğŸ˜ ì´ë ‡ê²Œ í•´ì„œ ë¼ë²¨ì„ ì •ë¦¬í•˜ì˜€ê³ , reviewë¥¼ ì •ë¦¬í•´ë³´ë„ë¡ í•˜ì.

### ìƒˆë¡œìš´ ì—´ ìƒì„±

ì–¸ì œë“ ì§€ customer reviewë¥¼ ë‹¤ë£° ë•Œ, ê° ë¦¬ë·°ì— ìˆëŠ” ë‹¨ì–´ì˜ ìˆ˜ë¥¼ ì²´í¬í•˜ëŠ” ê²ƒì„ ì¢‹ì€ ë°©ë²•ì´ë‹¤.
ë¦¬ë·°ê°€ "êµ‰ì¥í•´ìš”!"ì™€ ê°™ì´ í•˜ë‚˜ì˜ ë‹¨ì–´ì¼ ìˆ˜ë„ ìˆì§€ë§Œ, ëª‡ ì²œ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ í•˜ë‚˜ì˜ ì—ì„¸ì´ì¼ ìˆ˜ë„ ìˆë‹¤.
ê·¸ë˜ì„œ ì´ ê°ê°ì˜ ê²½ìš°ì— ëŒ€í•´ì„œ ì„œë¡œ ë‹¤ë¥´ê²Œ ë‹¤ë£¨ì–´ì•¼ í•œë‹¤.
ê° ë¦¬ë·°ì˜ ë‹¨ì–´ ìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ, ë§¤ìš° ê¸°ë³¸ì ì¸ ë°©ë²•ì¸ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ textë¥¼ ë‚˜ëˆ„ëŠ” ë°©ë²•ì„ í†µí•´ ì„¸ì–´ë³´ë„ë¡ í•˜ì.

ê° ë¦¬ë·°ì˜ ë‹¨ì–´ ìˆ˜ë¥¼ ì²´í¬í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ì •ì˜í•´ë³´ë„ë¡ í•˜ì.

``` python
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```

lower_condition() í•¨ìˆ˜ì™€ ë‹¬ë¦¬, compute_review_length()ëŠ” í‚¤ê°€ ë°ì´í„°ì…‹ì˜ ì—´ ì´ë¦„ ì¤‘ í•˜ë‚˜ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” dictionaryë¥¼ ë°˜í™˜í•œë‹¤.
ì´ ê²½ìš°ì—ëŠ”, compute_review_lenth()ê°€ Dataset.map()ì„ í†µê³¼í•  ë•Œ, ìƒˆë¡œìš´ review_length ì—´ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ë°ì´í„°ì…‹ì˜ ëª¨ë“  í–‰ì— ì ìš©ë  ê²ƒì´ë‹¤.

``` python
drug_dataset = drug_dataset.map(compute_length)
# ì²« ë²ˆì§¸ training ì˜ˆì‹œ ê²€ì‚¬
drug_dataset["train"][0]
```

``` python
{'patient_id': 206461,
 'drugName': 'Valsartan',
 'condition': 'left ventricular dysfunction',
 'review': '"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil"',
 'rating': 9.0,
 'date': 'May 20, 2012',
 'usefulCount': 27,
 'review_length': 17}
```

ì˜ˆìƒí–ˆë˜ ê²ƒì²˜ëŸ¼, training setì— review_length ì—´ì´ ì¶”ê°€ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
ì´ ìƒˆë¡œìš´ ì—´ì„ Dataset.sort()ë¥¼ ì‚¬ìš©í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì •ë ¬í•  ìˆ˜ ìˆë‹¤.

``` python
drug_dataset["train"].sort("review_length")[:3]
```

``` python
{'patient_id': [103488, 23627, 20558],
 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],
 'condition': ['birth control', 'muscle spasm', 'pain'],
 'review': ['"Excellent."', '"useless"', '"ok"'],
 'rating': [10.0, 1.0, 6.0],
 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],
 'usefulCount': [5, 2, 10],
 'review_length': [1, 1, 1]}
```

ì˜ˆìƒí•œ ëŒ€ë¡œ ì¼ë¶€ reviewì—ëŠ” í•œ ë‹¨ì–´ë§Œ í¬í•¨ë˜ì–´ ìˆì–´ ê°ì • ë¶„ì„ì—ëŠ” ê´œì°®ì„ ìˆ˜ ìˆì§€ë§Œ ìƒíƒœë¥¼ ì˜ˆì¸¡í•˜ë ¤ëŠ” ê²½ìš°ì—ëŠ” ìœ ìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.

ğŸ™‹â€â™‚ï¸ ë°ì´í„°ì…‹ì— ì—´ì„ ì¶”ê°€í•˜ëŠ” ë‹¤ë¥¸ ë°©ë²•ì€ Dataset.add_column() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.
ì´ í•¨ìˆ˜ëŠ” Python list ë˜ëŠ” NumPy ë°°ì—´ì„ ì œê³µí•˜ëŠ” ê²ƒì„ í—ˆë½í•´ì£¼ê³ , Dataset.map()ì´ ë¶„ì„ì— ì í•©í•˜ì§€ ì•Šì„ ë•Œ ì†ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.

ì´ì œ Dataset.filter() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ 30ê°œ ì´í•˜ì˜ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ” reviewë¥¼ ì œê±°í•´ë³´ë„ë¡ í•˜ì.
condition ì—´ì— í–ˆë˜ ê²ƒê³¼ ë¹„ìŠ·í•˜ê²Œ, ì¼ì • ê¸°ì¤€ì— ë§ëŠ” ê¸¸ì´ì˜ ë¦¬ë·°ê°€ ì•„ë‹Œ ë¦¬ë·°ë“¤ì„ í•„í„°ë§í•  ìˆ˜ ìˆë‹¤.

``` python
drug_dataset = drug_dataset.filter(lambda x: x["review_length"] > 30)
print(drug_dataset.num_rows)
```

``` python
{'train': 138514, 'test': 46108}
```

ê²°ê³¼ë¥¼ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯ì´, ê¸°ì¡´ì˜ training setê³¼ test setì—ì„œ ëŒ€ëµ 15% ì •ë„ì˜ ë¦¬ë·°ê°€ ì œê±°ë˜ì—ˆë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•  ê²ƒì€ ë¦¬ë·°ì— ì„ì—¬ ìˆëŠ” HTML characterë“¤ì´ë‹¤.
Pythonì˜ html ëª¨ë“ˆì„ ì‚¬ìš©í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì´ characterë“¤ì„ unescapeí•  ìˆ˜ ìˆë‹¤.

``` python
import html

text = "I&#039;m a transformer called BERT"
html.unescape(text)
```

``` python
"I'm a transformer called BERT"
```

Dataset.map()ì„ ì‚¬ìš©í•´ì„œ corpusì˜ ëª¨ë“  HTML characterë¥¼ unescapeí•˜ì˜€ë‹¤.

``` python
drug_dataset = drug_dataset.map(lambda x: {"review": html.unescape(x["review"])})
```

ì´ì™€ ê°™ì´ Dataset.map() methodëŠ” ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ ë§¤ìš° ìœ ìš©í•˜ë‹¤! ğŸ”¥
í•˜ì§€ë§Œ ì•„ì§ ìš°ë¦¬ëŠ” ë¹™ì‚°ì˜ ì¼ê° ë°–ì— ë³´ì§€ ì•Šì•˜ë‹¤!

### map() methodì˜ ìŠˆí¼íŒŒì›Œ ğŸ’ª

Dataset.map() methodëŠ” ì¼ê´„ ì²˜ë¦¬ëœ ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©° Trueë¡œ ì„¤ì •í•˜ë©´ í•œ ë²ˆì— map í•¨ìˆ˜ì— ì¼ê´„ ì˜ˆì œë¥¼ ë³´ë‚´ê²Œ ëœë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ì´ì „ì— ì‚¬ìš©í•œ map í•¨ìˆ˜ëŠ” ëª¨ë“  HTML chracrterì„ unescapeí•˜ëŠ”ë° ì¡°ê¸ˆì˜ ì‹œê°„ì´ ë“¤ì—ˆë‹¤.
list comprehensionì„ ì‚¬ìš©í•´ì„œ ì—¬ëŸ¬ ìš”ì†Œë¥¼ ë™ì‹œì— ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ ì´ ê³¼ì •ì˜ ì†ë„ë¥¼ ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆë‹¤.

batched=Trueë¡œ ëª…ì‹œí•˜ë©´ ë°ì´í„°ì…‹ì˜ í•„ë“œì™€ dictionaryë¥¼ í•¨ìˆ˜ê°€ ë°›ê²Œ ë˜ì§€ë§Œ, ê°ê°ì˜ ê°’ì€ í•˜ë‚˜ì˜ valueê°€ ì•„ë‹Œ, *list of values*ì´ë‹¤.
Dataset.map()ì˜ ë¦¬í„´ê°’ì€ ê¼­ ë˜‘ê°™ì•„ì•¼ë§Œ í•œë‹¤.
ì—…ë°ì´íŠ¸ í•˜ê±°ë‚˜ ë°ì´í„°ì…‹ì— ì¶”ê°€í•˜ë ¤ëŠ” í•„ë“œê°€ ìˆëŠ” dictionaryì™€ ê°’ ëª©ë¡ì²˜ëŸ¼ ë§ì´ë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒì˜ ì½”ë“œëŠ” batched=Trueë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë‹¤ë¥¸ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  HTML characterì„ unescapeí•œë‹¤.

``` python
new_drug_dataset = drug_dataset.map(
    lambda x: {"review": [html.unescape(o) for o in x["review"]]}, batched=True
)
```

ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ë©´, ì´ì „ì˜ ì½”ë“œë³´ë‹¤ ë”ìš± ë¹ ë¥´ê²Œ ì‹¤í–‰ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.
ì´ë¯¸ HTML-unescaped ë°ì´í„°ì—¬ì„œ ê·¸ëŸ° ê²ƒì´ ì•„ë‹ˆë¼, ì´ì „ ì„¹ì…˜ì˜ ë°ì´í„°ì— ëŒ€í•´ì„œ ì‹¤í–‰í•œë‹¤ê³  í•´ë„ ë˜‘ê°™ì€ ì‹œê°„ì´ ê±¸ë¦´ ê²ƒì´ë‹¤.
ì´ëŠ” ì™œëƒí•˜ë©´, list comprehensionì´ for ë£¨í”„ì—ì„œ ë˜‘ê°™ì€ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ë¹ ë¥´ê³ , ë™ì‹œì— ì—¬ëŸ¬ ìš”ì†Œì— ì ‘ê·¼í•¨ìœ¼ë¡œì¨ ì„±ëŠ¥ í–¥ìƒ ë˜í•œ ì–»ì„ ìˆ˜ ìˆë‹¤.

Chapter 6. ì—ì„œ ë‹¤ë£° ê±°ëŒ€í•œ text listì— ëŒ€í•´ì„œ ë¹ ë¥´ê²Œ í† í°í™”í•˜ëŠ” tokenizerì„ ì‚¬ìš©í•´ì„œ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ”, Dataset.mep()ì„ batched=Trueì™€ í•¨ê»˜ ì‚¬ìš©í•´ì•¼ë§Œ í•œë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, fast tokenizerë¡œ drug reviewë¥¼ í† í°í™”í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

``` python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["review"], truncation=True)
```

Chpater 3. ì—ì„œ ë´¤ë˜ ê²ƒì²˜ëŸ¼, í•œ ê°œ ë˜ëŠ” ì—¬ëŸ¬ ê°œì˜ exampleì„ tokenizerì— ë„£ì„ ìˆ˜ ìˆê³ , batched=Trueë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì‚¬ìš©í•˜ì§€ ì•Šê³  í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
ì„œë¡œ ë‹¤ë¥¸ ì˜µì…˜ì— ëŒ€í•´ì„œ ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ë„ë¡ í•˜ì.
ì½”ë“œë¥¼ ì‹¤í–‰í•  ë•Œ, %timeì„ ì½”ë“œ ì•ì— ì‚¬ìš©í•˜ë©´ ì‹¤í–‰ë˜ëŠ” ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì´ ìˆ˜ ìˆë‹¤!

``` python
%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)
```

ê·¸ë¦¬ê³  %%timeì„ cell ì‹œì‘ ë¶€ë¶„ì— ì¶”ê°€í•´ë‘ë©´ ëª¨ë“  ì…€ë¥¼ ì‹¤í–‰í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤.

ê²°ê³¼ë¥¼ í†µí•´ fast tokenizerì— batched=True ì˜µì…˜ì´ slow tokenizerì— batched=False ì˜µì…˜ë³´ë‹¤ 30ì´ˆ ê°€ëŸ‰ ë” ë¹ ë¥´ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
ì´ê²ƒì´ AutoTokenizerì˜ ê¸°ë³¸ê°’ì´ fast tokenizerì¸ ì´ìœ ì´ë‹¤!
ì´ë ‡ê²Œ ê°€ì†í™”ê°€ ê°€ëŠ¥í•œ ì´ìœ ëŠ” í† í°í™”ì˜ ë’·ë°°ê²½ì„ ë³´ë©´ Rustë¼ëŠ” ì½”ë“œë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰ì‹œì¼œì£¼ëŠ” ì–¸ì–´ë¡œ ì‹¤í–‰í•˜ê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•œ ê²ƒì´ë‹¤.

fast tokenizerì— batchë¥¼ í™œìš©í•˜ì˜€ì„ ë•Œ, 6ë°° ê°€ëŸ‰ ë¹ ë¥¸ ì†ë„ê°€ ë‚˜ì˜¨ ì´ìœ ì— ë³‘ë ¬í™” ë˜í•œ í¬í•¨ë˜ì–´ ìˆë‹¤.
ë‹¨ì¼ í† í°í™” ì‘ì—…ì„ ë³‘ë ¬í™”í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ ë™ì‹œì— ë§ì€ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ë ¤ëŠ” ê²½ìš° ê°ê° ìì²´ í…ìŠ¤íŠ¸ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ì„ ë¶„í• í•  ìˆ˜ ìˆë‹¤.

Dataset.map()ì€ ê·¸ ìì²´ì—ë„ ë³‘ë ¬í™”ë¥¼ í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ëŠ¥ë ¥ì´ ìˆë‹¤.
Rustì˜ ì§€ì›ì„ ë°›ì§€ ì•Šê¸° ë•Œë¬¸ì— slow tokenizerê°€ fast tokenizerë¥¼ ë”°ë¼ì¡ì§€ ëª»í•˜ê²Œ ë˜ì§€ë§Œ, ì—¬ì „íˆ ë„ì›€ì´ ëœë‹¤.
ë‹¤ì¤‘ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´, num_proc ì¸ìë¥¼ ì‚¬ìš©í•˜ê³  Dataset.map() í˜¸ì¶œì— ì‚¬ìš©í•  í”„ë¡œì„¸ìŠ¤ ìˆ˜ë¥¼ ì§€ì •í•´ì•¼ í•œë‹¤.

``` python
slow_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)

def slow_tokenize_function(examples):
    return slow_tokenizer(examples["review"], truncation=True)

tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)
```

ì‚¬ìš©í•  ìµœì ì˜ í”„ë¡œì„¸ìŠ¤ ìˆ˜ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ì•½ê°„ì˜ íƒ€ì´ë°ì„ ì‹¤í—˜í•  ìˆ˜ ìˆë‹¤.
ì‹¤í—˜ì˜ ê²½ìš°ì—ì„œëŠ” 8ì´ ìµœê³ ì˜ ì†ë„ ì´ë“ì„ ì–»ì–´ëƒˆë‹¤.

ì´ì œ ì´ê²ƒì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ì!
ìš°ë¦¬ì˜ exampleë“¤ì— ëŒ€í•´ í† í°í™”ë¥¼ í•˜ê³ , maximum lengthë¥¼ 128ë¡œ ì„¤ì •í•´ì„œ truncateë¥¼ í•˜ì˜€ë‹¤.
ê·¸ë¦¬ê³  tokenizerì—ê²Œ textì˜ ë§¨ ì²« ë²ˆì§¸ ë‹¨ì–´ ëŒ€ì‹ ì— ëª¨ë“  chunkë¥¼ ë°˜í™˜í•˜ë„ë¡ ìš”ì²­í•˜ì˜€ë‹¤.
ì´ëŠ” return_overflowing_tokens=Trueë¥¼ ì‚¬ìš©í•´ì„œ í•´ê²°í•  ìˆ˜ ìˆë‹¤!

``` python
def tokenize_and_split(examples):
    return tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
```

ì´ í•¨ìˆ˜ë¥¼ Dataset.map()ì„ í†µí•´ ì „ì²´ ë°ì´í„°ì…‹ì— ì‚¬ìš©í•˜ê¸° ì „ì—, í•˜ë‚˜ì˜ exampleì— ëŒ€í•´ ì‚¬ìš©í•´ë³´ì.

``` python
result = tokenize_and_split(drug_dataset["train"][0])
[len(inp) for inp in result["input_ids"]]
```

``` python
[128, 49]
```

ê²°ê³¼ëŠ” ë‘ ê°œì˜ featureë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.
ì²« ë²ˆì§¸ ê°’ì€ maximum lengthë¡œ ì„¤ì •í•´ë‘” 128ìœ¼ë¡œ truncate ë˜ì—ˆê³ , ë‘ ë²ˆì§¸ ê°’ì€ 49ì˜ ê¸¸ì´ë¥¼ ê°€ì§„ ë¬¸ì¥ì´ë‹¤.
ì´ì œ ì´ í•¨ìˆ˜ë¥¼ ë°ì´í„°ì…‹ì˜ ëª¨ë“  ìš”ì†Œì— ëŒ€í•´ ì ìš©í•´ë³´ì!

``` python
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
```

``` python
ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000
```

í•˜ì§€ë§Œ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤! ğŸ˜…
ë­ê°€ ë¬¸ì œì¼ê¹Œ? ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ë‹¨ì„œë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ”ë°, ì—´ì˜ ê¸¸ì´ ê°„ì˜ ë¯¸ìŠ¤ë§¤ì¹˜ê°€ ë°œìƒí•˜ì˜€ê¸° ë•Œë¬¸ì´ë‹¤.
í•˜ë‚˜ëŠ” 1,463ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” 1,000ì´ê¸° ë•Œë¬¸ì´ë‹¤.
Dataset.map() ë¬¸ì„œë¥¼ ë³¸ ì ì´ ìˆë‹¤ë©´ mappingí•˜ëŠ” í•¨ìˆ˜ì— ì „ë‹¬ëœ ìƒ˜í”Œì˜ ìˆ˜ë¼ëŠ” ê²ƒì„ ê¸°ì–µí•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ 1,000ê°œì˜ ì˜ˆëŠ” 1,463ê°œì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ shape errorê°€ ë°œìƒí•œ ê²ƒì´ë‹¤.

ê²°êµ­ ë¬¸ì œëŠ” ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ë¥¼ ê°€ì§„ ë°ì´í„°ì…‹ì„ ì„ìœ¼ë ¤ í•œ ê²ƒì´ë‹¤.
drug_dataset ì—´ì€ íŠ¹ì • ê°œìˆ˜ì˜ exampleì„ ê°€ì§€ì§€ë§Œ, ì–»ê²Œ ëœ tokenizer_datasetì€ ë” ë§ì€ exampleì„ ê°€ì§€ê¸° ë•Œë¬¸ì´ë‹¤.
ì´ë ‡ê²Œ ë˜ë©´ Datasetì—ì„œ ì‘ë™í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ì´ì „ ë°ì´í„°ì…‹ì—ì„œ ì—´ì„ ì œê±°í•˜ê±°ë‚˜ ìƒˆ ë°ì´í„°ì…‹ê³¼ ë™ì¼í•œ í¬ê¸°ë¡œ ë§Œë“¤ì–´ì•¼ í•œë‹¤.
remove_columnsë¥¼ ì‚¬ìš©í•´ì„œ ì „ìë¥¼ í•  ìˆ˜ ìˆë‹¤!

``` python
tokenized_dataset = drug_dataset.map(
    tokenize_and_split, batched=True, remove_columns=drug_dataset["train"].column_names
)
```

ì´ì œ ì´ ì½”ë“œëŠ” ì˜¤ë¥˜ ì—†ì´ ì˜ ì‘ë™í•œë‹¤.
ê¸¸ì´ë¥¼ ë¹„êµí•´ë³´ë©´, ìƒˆ ë°ì´í„°ì…‹ì€ ê¸°ì¡´ì˜ ë°ì´í„°ì…‹ë³´ë‹¤ ë” ë§ì€ ìš”ì†Œë¥¼ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

``` python
len(tokenized_dataset["train"]), len(drug_dataset["train"])
```

``` python
(206772, 138514)
```

ì•ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ ê¸¸ì´ì˜ ë¯¸ìŠ¤ë§¤ì¹˜ëŠ” ì˜¤ë˜ëœ ì—´ì„ ìƒˆë¡œìš´ ì—´ê³¼ ë˜‘ê°™ì´ ë§ì¶¤ìœ¼ë¡œì¨ í•´ê²°í•  ìˆ˜ ìˆë‹¤.
ì´ë¥¼ ìœ„í•´ì„œ, return_overflowing_tokens=Trueë¥¼ ì„¤ì •í•  ë•Œ, tokenizerê°€ ë°˜í™˜í•˜ëŠ”overflow_to_sample_mapping í•„ë“œê°€ í•„ìš”í•˜ë‹¤.
ì´ê²ƒì€ ìƒˆ ê¸°ëŠ¥ indexì—ì„œ ì›ë˜ ìƒ˜í”Œ indexë¡œì˜ mappingì„ ì œê³µí•œë‹¤.
ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆ ê¸°ëŠ¥ì„ ìƒì„±í•˜ëŠ” ë§Œí¼ ê° ì˜ˆì œì˜ ê°’ì„ ë°˜ë³µí•˜ì—¬ ì›ë˜ ë°ì´í„°ì…‹ì— ìˆëŠ” ê° í‚¤ë¥¼ ì ì ˆí•œ í¬ê¸°ì˜ value listê³¼ ì—°ê²°í•  ìˆ˜ ìˆë‹¤.

``` python
def tokenize_and_split(examples):
    result = tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
    # ìƒˆë¡œìš´ ê²ƒê³¼ ì˜¤ë˜ëœ ê²ƒ ì‚¬ì´ì—ì„œ ë§¤í•‘ ì¶”ì¶œ
    sample_map = result.pop("overflow_to_sample_mapping")
    for key, values in examples.items():
        result[key] = [values[i] for i in sample_map]
    return result
```

ì˜¤ë˜ëœ ì—´ì„ ì œê±°í•  í•„ìš”ì—†ì´ Dataset.map()ê³¼ í•¨ê»˜ ì‘ë™í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

``` python
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
tokenized_dataset
```

``` python
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 206772
    })
    test: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 68876
    })
})
```

ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë˜‘ê°™ì€ ìˆ˜ì˜ training featureì„ ì–»ì—ˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ëª¨ë“  ì´ì „ í•„ë“œë¥¼ ìœ ì§€í•˜ê³  ìˆë‹¤.
ë§Œì•½ ì´ëŸ¬í•œ í•„ë“œë“¤ì´ ëª¨ë¸ì— ì ìš©í•œ í›„ í›„ì²˜ë¦¬ ì‘ì—…ì— í•„ìš”í•˜ë‹¤ë©´, ì´ëŸ¬í•œ ë°©ì‹ì„ ì‚¬ìš©í•˜ê¸¸ ë°”ë€ë‹¤.

ì§€ê¸ˆê¹Œì§€ ğŸ¤— Datasets ê°€ ì–´ë–»ê²Œ ë°ì´í„°ì…‹ì„ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì „ì²˜ë¦¬í•˜ëŠ”ì§€ ì‚´í´ë³´ì•˜ë‹¤.
ğŸ¤— Datasets ì˜ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ì´ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ ëŒ€ë¶€ë¶„ì˜ ë¶„ì•¼ëŠ” ì»¤ë²„ ê°€ëŠ¥í•˜ê² ì§€ë§Œ, ë”ìš± ê°•ë ¥í•œ ì»¨íŠ¸ë¡¤ì´ í•„ìš”í•˜ë‹¤ë©´ Pandasì™€ ìœµí•©í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆê¸¸ ë°”ë€ë‹¤.
ë‹¤í–‰ìŠ¤ëŸ½ê²Œë„, ğŸ¤— Datasets ëŠ” Pythonì˜ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì¸ Pandas, NumPy, PyTorch, TensorFlow, ê·¸ë¦¬ê³  JAX ë“±ê³¼ í˜¸í™˜ì´ ë˜ë„ë¡ ì„¤ê³„ ë˜ì—ˆë‹¤!
ì´ì œ ì´ê²ƒë“¤ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‚´í´ë³´ë„ë¡ í•˜ì.

### Datasetì—ì„œ DataFrameìœ¼ë¡œ ëŒì•„ê°€ê¸°


