# Slice & Dice

앞으로도 많은 task를 진행할텐데, 대부분의 경우에는 이 데이터가 model에 적합하지 않은 형태일 것이다.
그래서 이번 섹션에서는 🤗 Datasets 라이브러리가 제공하는 dataset cleaning feature들을 살펴보도록 하겠다.

### Slicing & Dicing data

Pandas와 유사하게, 🤗 Datasets는 Dataset과 DatasetDict object에 대해서 조종할 수 있는 다양한 함수를 제공한다.
이미, Chapter 3. 에서 우리는 Dataset.map() method를 경험하였다.
따라서 이번 섹션에서는 이 외에 더욱 다양한 함수들에 대해 알아보도록 하겠다! 🔥

예시를 보여주기 위해 사용할 데이터셋은 [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29)으로 진행하겠다.
이 데이터셋은 다양한 약품에 대한 환자들의 리뷰와 약을 복용했을 때 컨디션의 변화를 10점 만점으로 해서 평가하고 있다.

가장 처음으로 해야할 것은 데이터를 다운로드하고 추출하는 것이다.
이는 wget과 unzip 명령어를 사용해서 할 수 있다.

``` python
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
!unzip drugsCom_raw.zip
```

TSV는 CSV의 변형으로 tab을 separator로 받는다.
이러한 형식의 파일을 불러올 때는, csv를 불러올 때랑 똑같은데 load_dataset()의 delimiter 인자를 다음과 같이 명시해서 사용하면 된다.

``` python
from datasets import load_dataset

data_files = {"train": "drugsComTrain_raw.tsv", "test": "drugsComTest_raw.tsv"}
# \t는 Python에서 tab을 의미한다.
drug_dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
```

어떠한 종류의 데이터로 작업을 하든, 그 데이터의 유형에 대해 빠르게 확인하는 방법은 랜덤하게 소량의 데이터를 추출해서 확인해보는 것이다.
🤗 Datasets에서는, Dataset.shuffle()과 Dataset.select() 함수를 연결해서 랜덤하게 샘플링할 수 있다.

``` python
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
# 처음 몇 개의 예시 살펴보기
drug_sample[:3]
```

``` python
{'Unnamed: 0': [87571, 178045, 80482],
 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],
 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],
 'review': ['"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"',
  '"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."',
  '"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."'],
 'rating': [9.0, 3.0, 10.0],
 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],
 'usefulCount': [36, 13, 128]}
```

재현성을 위해서 Dataset.shuffle()의 seed를 수정하였다.
Dataset.select()는 반복적인 index를 예상하므로 range(1000)을 전달하여 셔플된 데이터셋에서 처음 1,000개의 예제를 가져왔다.
이 데이터셋에서 나온 샘플에서 우리는 이미 몇 개의 이상한 점을 파악할 수 있다.

- Unnamed: 0 열은 각 환자에 대해 익명화된 ID처럼 의심스럽게 보인다.
- condition 열은 대문자와 소문자 라벨을 초함하고 있다.
- 리뷰들은 서로 다른 다양한 길이와 Python의 line separator(\r \n) 뿐만 아니라 HTML의 character code(&\#039)의 조합으로 이루어져 있다.

위에서 나열한 각각의 이슈들에 대해서 어떻게 🤗 Datasets를 사용할 수 있는지 알아보자.
Unnamed: 0 열에 대한 환자 ID 가설을 테스트하려면, Dataset.unique() 함수를 사용해서 ID 수가 각 split의 행 수와 일치하는 지 확인하면 된다.

``` python
for split in drug_dataset.keys():
    assert len(drug_dataset[split]) == len(drug_dataset[split].unique("Unnamed: 0"))
```

이렇게 하면 이 가설을 확인하는 것처럼 보인다.
그렇다면 Unnamed: 0 열을 좀 더 해석 가능한 이름으로 변경해서, 데이터셋을 좀 더 정리해보자.
DatasetDict.rename_column() 함수를 사용해서 두 split의 열의 이름을 한 번에 변경할 수 있다.

``` python
drug_dataset = drug_dataset.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
drug_dataset
```

``` python
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 161297
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 53766
    })
})
```

다음으로, Dataset.map()을 사용해서 condition 라벨을 정규화해보자.
Chapter 3. 에서 tokenization으로 했던 것처럼, drug_dataset의 각 split의 모든 행에 대해 적용될 수 있는 간단한 함수를 정의해보자.

``` python
def lowercase_condition(example):
    return {"condition": example["condtion"].lower()}
    
drug_dataset.map(lowercase_condition)
```

``` python
AttributeError: 'NoneType' object has no attribute 'lower'
```

하지만 map 함수에 대해 다음과 같은 오류가 발생하는 것을 볼 수 있다.. 😢
이 오류로부터 condition 열에 문자열로 소문자 변환이 불가능한 None 값이 있다는 것을 추론할 수 있었다.
Dataset.mep()과 비슷한 방식으로 작동하고, 데이터셋의 한 가지 예시를 받는 함수인 Dataset.filter()을 사용해서 None값을 가지는 행을 삭제해보자.

``` python
def filter_noise(x):
    return x["condition"] is not None
```

그 다음에, drug_dataset.filter(filter_nones)를 *lambda* 함수를 사용해서 한 줄로 실행할 수 있다.
Python에서는, lambda 함수를 딱히 명명하지 않고 작은 함수처럼 사용할 수 있다.
일반적으로 다음과 같이 사용한다.

``` python
lambda <arguments> : <expression>
```

여기서 lambda는 Python의 특별한 키워드이고, <arguments>는 쉼표를 기준으로 나눠지는 값으로 함수에 대한 입력을 정의하고, <expression>은 실행할 연산을 표현한다.
예를 들어, 숫자를 제곱하는 간단한 연산을 lambda를 사용하여 다음과 같이 정의할 수 있다.
  
``` python
lambda x : x * x
```

이 함수를 입력에 적용하기 위해, 이 함수와 입력을 괄호로 감싸야 한다.

``` python
(lambda x: x * x)(3)
```

``` python
9
```

이와 비슷하게, 쉼표로 나누어서 lambda 함수를 여러 개의 인자로 정의할 수 있다.
예를 들어, 삼각형의 넓이를 구하는 방법을 다음과 같은 계산할 수 있다.

``` python
(lambda base, height: 0.5 * base * height)(4, 8)
```

``` python
16.0
```

lambda 함수는 작고, 한 번만 사용할 참수를 정의할 때 유용하게 사용된다.
🤗 Datasets 문맥에서는, lambda 함수를 사용해서 간단한 map과 filter 연산을 정의할 수 있다.
그래서, 이 트릭을 사용해서 데이터셋에서 None값을 제거할 수 있다.

``` python
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None)
```

None값이 제거됨과 함께, condition 열을 정규화할 수 있다.

``` python
drug_dataset = drug_dataset.map(lowercase_condition)
# 소문자화가 잘 되었는지 확인하기
drug_dataset["train"]["condition"][:3]
```

``` python
['left venticular dysfunction', 'adhd', 'birth control']
```

잘 작동되었다! 😁 이렇게 해서 라벨을 정리하였고, review를 정리해보도록 하자.

### 새로운 열 생성

언제든지 customer review를 다룰 때, 각 리뷰에 있는 단어의 수를 체크하는 것을 좋은 방법이다.
리뷰가 "굉장해요!"와 같이 하나의 단어일 수도 있지만, 몇 천 단어로 이루어진 하나의 에세이일 수도 있다.
그래서 이 각각의 경우에 대해서 서로 다르게 다루어야 한다.
각 리뷰의 단어 수를 계산하기 위해서, 매우 기본적인 방법인 공백을 기준으로 text를 나누는 방법을 통해 세어보도록 하자.

각 리뷰의 단어 수를 체크하는 간단한 함수를 정의해보도록 하자.

``` python
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```

lower_condition() 함수와 달리, compute_review_length()는 키가 데이터셋의 열 이름 중 하나와 일치하지 않는 dictionary를 반환한다.
이 경우에는, compute_review_lenth()가 Dataset.map()을 통과할 때, 새로운 review_length 열을 생성하기 위해 데이터셋의 모든 행에 적용될 것이다.

``` python
drug_dataset = drug_dataset.map(compute_length)
# 첫 번째 training 예시 검사
drug_dataset["train"][0]
```

``` python
{'patient_id': 206461,
 'drugName': 'Valsartan',
 'condition': 'left ventricular dysfunction',
 'review': '"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil"',
 'rating': 9.0,
 'date': 'May 20, 2012',
 'usefulCount': 27,
 'review_length': 17}
```

예상했던 것처럼, training set에 review_length 열이 추가된 것을 확인할 수 있다.
이 새로운 열을 Dataset.sort()를 사용해서 다음과 같이 정렬할 수 있다.

``` python
drug_dataset["train"].sort("review_length")[:3]
```

``` python
{'patient_id': [103488, 23627, 20558],
 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],
 'condition': ['birth control', 'muscle spasm', 'pain'],
 'review': ['"Excellent."', '"useless"', '"ok"'],
 'rating': [10.0, 1.0, 6.0],
 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],
 'usefulCount': [5, 2, 10],
 'review_length': [1, 1, 1]}
```

예상한 대로 일부 review에는 한 단어만 포함되어 있어 감정 분석에는 괜찮을 수 있지만 상태를 예측하려는 경우에는 유용하지 않을 수 있다.

🙋‍♂️ 데이터셋에 열을 추가하는 다른 방법은 Dataset.add_column() 함수를 사용하는 것이다.
이 함수는 Python list 또는 NumPy 배열을 제공하는 것을 허락해주고, Dataset.map()이 분석에 적합하지 않을 때 손쉽게 사용할 수 있게 해준다.

이제 Dataset.filter() 함수를 사용해서 30개 이하의 단어로 이루어진 단어를 포함하고 있는 review를 제거해보도록 하자.
condition 열에 했던 것과 비슷하게, 일정 기준에 맞는 길이의 리뷰가 아닌 리뷰들을 필터링할 수 있다.

``` python
drug_dataset = drug_dataset.filter(lambda x: x["review_length"] > 30)
print(drug_dataset.num_rows)
```

``` python
{'train': 138514, 'test': 46108}
```

결과를 보면 알 수 있듯이, 기존의 training set과 test set에서 대략 15% 정도의 리뷰가 제거되었다.

마지막으로 처리해야 할 것은 리뷰에 섞여 있는 HTML character들이다.
Python의 html 모듈을 사용해서 다음과 같이 이 character들을 unescape할 수 있다.

``` python
import html

text = "I&#039;m a transformer called BERT"
html.unescape(text)
```

``` python
"I'm a transformer called BERT"
```

Dataset.map()을 사용해서 corpus의 모든 HTML character를 unescape하였다.

``` python
drug_dataset = drug_dataset.map(lambda x: {"review": html.unescape(x["review"])})
```

이와 같이 Dataset.map() method는 데이터를 처리할 때 매우 유용하다! 🔥
하지만 아직 우리는 빙산의 일각 밖에 보지 않았다!

### map() method의 슈퍼파워 💪

Dataset.map() method는 일괄 처리된 인수를 사용하며 True로 설정하면 한 번에 map 함수에 일괄 예제를 보내게 된다.
예를 들어, 이전에 사용한 map 함수는 모든 HTML chracrter을 unescape하는데 조금의 시간이 들었다.
list comprehension을 사용해서 여러 요소를 동시에 처리함으로써 이 과정의 속도를 빠르게 할 수 있다.

batched=True로 명시하면 데이터셋의 필드와 dictionary를 함수가 받게 되지만, 각각의 값은 하나의 value가 아닌, *list of values*이다.
Dataset.map()의 리턴값은 꼭 똑같아야만 한다.
업데이트 하거나 데이터셋에 추가하려는 필드가 있는 dictionary와 값 목록처럼 말이다.
예를 들어, 다음의 코드는 batched=True를 사용하지만, 다른 방법을 사용하여 모든 HTML character을 unescape한다.

``` python
new_drug_dataset = drug_dataset.map(
    lambda x: {"review": [html.unescape(o) for o in x["review"]]}, batched=True
)
```

이 코드를 실행해보면, 이전의 코드보다 더욱 빠르게 실행된다는 것을 알 수 있을 것이다.
이미 HTML-unescaped 데이터여서 그런 것이 아니라, 이전 섹션의 데이터에 대해서 실행한다고 해도 똑같은 시간이 걸릴 것이다.
이는 왜냐하면, list comprehension이 for 루프에서 똑같은 코드를 실행하는 것보다 더 빠르고, 동시에 여러 요소에 접근함으로써 성능 향상 또한 얻을 수 있다.

Chapter 6. 에서 다룰 거대한 text list에 대해서 빠르게 토큰화하는 tokenizer을 사용해서 속도를 향상시키기 위해서는, Dataset.mep()을 batched=True와 함께 사용해야만 한다.
예를 들어, fast tokenizer로 drug review를 토큰화하기 위해서는 다음과 같이 사용해야 한다.

``` python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["review"], truncation=True)
```

Chpater 3. 에서 봤던 것처럼, 한 개 또는 여러 개의 example을 tokenizer에 넣을 수 있고, batched=True를 사용하거나 사용하지 않고 함수를 사용할 수 있다.
서로 다른 옵션에 대해서 성능을 비교해보도록 하자.
코드를 실행할 때, %time을 코드 앞에 사용하면 실행되는 걸리는 시간을 잴 수 있다!

``` python
%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)
```

그리고 %%time을 cell 시작 부분에 추가해두면 모든 셀를 실행하는데 걸리는 시간을 측정할 수 있다.

결과를 통해 fast tokenizer에 batched=True 옵션이 slow tokenizer에 batched=False 옵션보다 30초 가량 더 빠르다는 것을 알 수 있다.
이것이 AutoTokenizer의 기본값이 fast tokenizer인 이유이다!
이렇게 가속화가 가능한 이유는 토큰화의 뒷배경을 보면 Rust라는 코드를 병렬로 실행시켜주는 언어로 실행하기 때문에 가능한 것이다.

fast tokenizer에 batch를 활용하였을 때, 6배 가량 빠른 속도가 나온 이유에 병렬화 또한 포함되어 있다.
단일 토큰화 작업을 병렬화할 수는 없지만 동시에 많은 텍스트를 토큰화하려는 경우 각각 자체 텍스트를 담당하는 여러 프로세스로 실행을 분할할 수 있다.

Dataset.map()은 그 자체에도 병렬화를 할 수 있게 해주는 능력이 있다.
Rust의 지원을 받지 않기 때문에 slow tokenizer가 fast tokenizer를 따라잡지 못하게 되지만, 여전히 도움이 된다.
다중 처리를 가능하게 하기 위해, num_proc 인자를 사용하고 Dataset.map() 호출에 사용할 프로세스 수를 지정해야 한다.

``` python
slow_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)

def slow_tokenize_function(examples):
    return slow_tokenizer(examples["review"], truncation=True)

tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)
```

사용할 최적의 프로세스 수를 결정하기 위해 약간의 타이밍을 실험할 수 있다.
실험의 경우에서는 8이 최고의 속도 이득을 얻어냈다.

이제 이것이 어떻게 작동하는지 확인해보자!
우리의 example들에 대해 토큰화를 하고, maximum length를 128로 설정해서 truncate를 하였다.
그리고 tokenizer에게 text의 맨 첫 번째 단어 대신에 모든 chunk를 반환하도록 요청하였다.
이는 return_overflowing_tokens=True를 사용해서 해결할 수 있다!

``` python
def tokenize_and_split(examples):
    return tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
```

이 함수를 Dataset.map()을 통해 전체 데이터셋에 사용하기 전에, 하나의 example에 대해 사용해보자.

``` python
result = tokenize_and_split(drug_dataset["train"][0])
[len(inp) for inp in result["input_ids"]]
```

``` python
[128, 49]
```

결과는 두 개의 feature로 구성되어 있다.
첫 번째 값은 maximum length로 설정해둔 128으로 truncate 되었고, 두 번째 값은 49의 길이를 가진 문장이다.
이제 이 함수를 데이터셋의 모든 요소에 대해 적용해보자!

``` python
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
```

``` python
ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000
```

하지만 작동하지 않는다! 😅
뭐가 문제일까? 에러 메시지에서 단서를 얻을 수 있는데, 열의 길이 간의 미스매치가 발생하였기 때문이다.
하나는 1,463이고, 다른 하나는 1,000이기 때문이다.
Dataset.map() 문서를 본 적이 있다면 mapping하는 함수에 전달된 샘플의 수라는 것을 기억할 수 있다. 여기서 1,000개의 예는 1,463개의 새로운 기능을 제공하여 shape error가 발생한 것이다.

결국 문제는 두 개의 서로 다른 크기를 가진 데이터셋을 섞으려 한 것이다.
drug_dataset 열은 특정 개수의 example을 가지지만, 얻게 된 tokenizer_dataset은 더 많은 example을 가지기 때문이다.
이렇게 되면 Dataset에서 작동하지 않기 때문에, 이전 데이터셋에서 열을 제거하거나 새 데이터셋과 동일한 크기로 만들어야 한다.
remove_columns를 사용해서 전자를 할 수 있다!

``` python
tokenized_dataset = drug_dataset.map(
    tokenize_and_split, batched=True, remove_columns=drug_dataset["train"].column_names
)
```

이제 이 코드는 오류 없이 잘 작동한다.
길이를 비교해보면, 새 데이터셋은 기존의 데이터셋보다 더 많은 요소를 가지고 있다는 것을 확인할 수 있다.

``` python
len(tokenized_dataset["train"]), len(drug_dataset["train"])
```

``` python
(206772, 138514)
```

앞서 언급한 것처럼 길이의 미스매치는 오래된 열을 새로운 열과 똑같이 맞춤으로써 해결할 수 있다.
이를 위해서, return_overflowing_tokens=True를 설정할 때, tokenizer가 반환하는overflow_to_sample_mapping 필드가 필요하다.
이것은 새 기능 index에서 원래 샘플 index로의 mapping을 제공한다.
이를 사용하여 새 기능을 생성하는 만큼 각 예제의 값을 반복하여 원래 데이터셋에 있는 각 키를 적절한 크기의 value list과 연결할 수 있다.

``` python
def tokenize_and_split(examples):
    result = tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
    # 새로운 것과 오래된 것 사이에서 매핑 추출
    sample_map = result.pop("overflow_to_sample_mapping")
    for key, values in examples.items():
        result[key] = [values[i] for i in sample_map]
    return result
```

오래된 열을 제거할 필요없이 Dataset.map()과 함께 작동하는 것을 볼 수 있다.

``` python
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
tokenized_dataset
```

``` python
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 206772
    })
    test: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 68876
    })
})
```

이전과 마찬가지로 똑같은 수의 training feature을 얻었지만, 여기서는 모든 이전 필드를 유지하고 있다.
만약 이러한 필드들이 모델에 적용한 후 후처리 작업에 필요하다면, 이러한 방식을 사용하길 바란다.

지금까지 🤗 Datasets 가 어떻게 데이터셋을 다양한 방법으로 전처리하는지 살펴보았다.
🤗 Datasets 의 처리 함수들이 데이터셋에 대해서 대부분의 분야는 커버 가능하겠지만, 더욱 강력한 컨트롤이 필요하다면 Pandas와 융합하여 사용할 수 있길 바란다.
다행스럽게도, 🤗 Datasets 는 Python의 다른 라이브러리들인 Pandas, NumPy, PyTorch, TensorFlow, 그리고 JAX 등과 호환이 되도록 설계 되었다!
이제 이것들이 어떻게 작동하는지 살펴보도록 하자.

### Dataset에서 DataFrame으로 돌아가기


