# Slice & Dice

앞으로도 많은 task를 진행할텐데, 대부분의 경우에는 이 데이터가 model에 적합하지 않은 형태일 것이다.
그래서 이번 섹션에서는 🤗 Datasets 라이브러리가 제공하는 dataset cleaning feature들을 살펴보도록 하겠다.

### Slicing & Dicing data

Pandas와 유사하게, 🤗 Datasets는 Dataset과 DatasetDict object에 대해서 조종할 수 있는 다양한 함수를 제공한다.
이미, Chapter 3. 에서 우리는 Dataset.map() method를 경험하였다.
따라서 이번 섹션에서는 이 외에 더욱 다양한 함수들에 대해 알아보도록 하겠다! 🔥

예시를 보여주기 위해 사용할 데이터셋은 [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29)으로 진행하겠다.
이 데이터셋은 다양한 약품에 대한 환자들의 리뷰와 약을 복용했을 때 컨디션의 변화를 10점 만점으로 해서 평가하고 있다.

가장 처음으로 해야할 것은 데이터를 다운로드하고 추출하는 것이다.
이는 wget과 unzip 명령어를 사용해서 할 수 있다.

``` python
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
!unzip drugsCom_raw.zip
```

TSV는 CSV의 변형으로 tab을 separator로 받는다.
이러한 형식의 파일을 불러올 때는, csv를 불러올 때랑 똑같은데 load_dataset()의 delimiter 인자를 다음과 같이 명시해서 사용하면 된다.

``` python
from datasets import load_dataset

data_files = {"train": "drugsComTrain_raw.tsv", "test": "drugsComTest_raw.tsv"}
# \t는 Python에서 tab을 의미한다.
drug_dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
```

어떠한 종류의 데이터로 작업을 하든, 그 데이터의 유형에 대해 빠르게 확인하는 방법은 랜덤하게 소량의 데이터를 추출해서 확인해보는 것이다.
🤗 Datasets에서는, Dataset.shuffle()과 Dataset.select() 함수를 연결해서 랜덤하게 샘플링할 수 있다.

``` python
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
# 처음 몇 개의 예시 살펴보기
drug_sample[:3]
```

``` python
{'Unnamed: 0': [87571, 178045, 80482],
 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],
 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],
 'review': ['"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"',
  '"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."',
  '"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."'],
 'rating': [9.0, 3.0, 10.0],
 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],
 'usefulCount': [36, 13, 128]}
```

재현성을 위해서 Dataset.shuffle()의 seed를 수정하였다.
Dataset.select()는 반복적인 index를 예상하므로 range(1000)을 전달하여 셔플된 데이터셋에서 처음 1,000개의 예제를 가져왔다.
이 데이터셋에서 나온 샘플에서 우리는 이미 몇 개의 이상한 점을 파악할 수 있다.

- Unnamed: 0 열은 각 환자에 대해 익명화된 ID처럼 의심스럽게 보인다.
- condition 열은 대문자와 소문자 라벨을 초함하고 있다.
- 리뷰들은 서로 다른 다양한 길이와 Python의 line separator(\r \n) 뿐만 아니라 HTML의 character code(&\#039)의 조합으로 이루어져 있다.

위에서 나열한 각각의 이슈들에 대해서 어떻게 🤗 Datasets를 사용할 수 있는지 알아보자.
Unnamed: 0 열에 대한 환자 ID 가설을 테스트하려면, Dataset.unique() 함수를 사용해서 ID 수가 각 split의 행 수와 일치하는 지 확인하면 된다.

``` python
for split in drug_dataset.keys():
    assert len(drug_dataset[split]) == len(drug_dataset[split].unique("Unnamed: 0"))
```

이렇게 하면 이 가설을 확인하는 것처럼 보인다.
그렇다면 Unnamed: 0 열을 좀 더 해석 가능한 이름으로 변경해서, 데이터셋을 좀 더 정리해보자.
DatasetDict.rename_column() 함수를 사용해서 두 split의 열의 이름을 한 번에 변경할 수 있다.

``` python
drug_dataset = drug_dataset.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
drug_dataset
```

``` python
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 161297
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 53766
    })
})
```

다음으로, Dataset.map()을 사용해서 condition 라벨을 정규화해보자.
Chapter 3. 에서 tokenization으로 했던 것처럼, drug_dataset의 각 split의 모든 행에 대해 적용될 수 있는 간단한 함수를 정의해보자.

``` python
def lowercase_condition(example):
    return {"condition": example["condtion"].lower()}
    
drug_dataset.map(lowercase_condition)
```

``` python
AttributeError: 'NoneType' object has no attribute 'lower'
```

하지만 map 함수에 대해 다음과 같은 오류가 발생하는 것을 볼 수 있다.. 😢
이 오류로부터 condition 열에 문자열로 소문자 변환이 불가능한 None 값이 있다는 것을 추론할 수 있었다.
Dataset.mep()과 비슷한 방식으로 작동하고, 데이터셋의 한 가지 예시를 받는 함수인 Dataset.filter()을 사용해서 None값을 가지는 행을 삭제해보자.

``` python
def filter_noise(x):
    return x["condition"] is not None
```

그 다음에, drug_dataset.filter(filter_nones)를 *lambda* 함수를 사용해서 한 줄로 실행할 수 있다.
Python에서는, lambda 함수를 딱히 명명하지 않고 작은 함수처럼 사용할 수 있다.
일반적으로 다음과 같이 사용한다.

``` python
lambda <arguments> : <expression>
```

여기서 lambda는 Python의 특별한 키워드이고, <arguments>는 쉼표를 기준으로 나눠지는 값으로 함수에 대한 입력을 정의하고, <expression>은 실행할 연산을 표현한다.
예를 들어, 숫자를 제곱하는 간단한 연산을 lambda를 사용하여 다음과 같이 정의할 수 있다.
  
``` python
lambda x : x * x
```

