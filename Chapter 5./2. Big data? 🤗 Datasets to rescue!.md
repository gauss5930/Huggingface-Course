# Big data? 🤗 Datasets to the rescue!

요즘에는 multi-gigabyte의 데이터셋으로 작업을 하는 사례가 거의 드물다. 
특히, BERT와 GPT-2와 같은 transformer을 밑바닥부터 사전 학습 시킨다면 말이다.
이러한 경우에는, data를 불러오는 것만 해도 엄청 어렵다.
예를 들어, GPT-2를 pre-trian하는데 사용되는 WebGPT의 corpus는 800만 개 이상의 문서와 40 GB의 text로 이루어져 있다.
이렇게 거대한 데이터를 당신 노트북의 RAM으로 불러온다는 것은 노트북을 죽이겠다는 것과 마찬가지다!! 😵

다행히도, 🤗 Datasets는 이러한 제약을 극복할 수 있도록 설계되었다.
이 라이브러리는 데이터셋을 memory-mapped 파일로 다뤄서, memory management 문제에서 벗어날 수 있도록 하였고, 코퍼스의 항목을 스트리밍하여 하드 드라이브 제한에서 벗어나게 하였다.

이번 섹션에서는 🤗 Datasets의 이러한 특징으로 825 GB의 corpus를 가지고 있는 'Pile'에 대해 알아볼 것이다.
자 드가자~!! 🔥

### Pile이 무엇일까?

Pile은 EleutherAI에서 생성한 English text corpus로, 큰 규모의 LM을 학습시키기 위해 만들어졌다.
이 데이터셋은 매우 다양한 분야의 데이터를 가지고 있다.
사용 가능한 학습 데이타만 해도 14 GB의 chunk로 이루어져 있고, 각각의 요소 또한 다운로드할 수 있다.
우선 PubMed Abstracts 데이터셋부터 살펴보자.
이 데이터셋은 PubMed에서 나온 1,500만 개의 biomedical 저작물의 abstract corpus이다.
데이터셋은 JSON Lines format으로 zstandard 라이브러리를 사용해서 함축되었기 때문에, 우선 이것을 install 해야 한다.

``` python
!pip install zstandard
```

다음으로, 이전 섹션에서 배웠던 멀리 떨어져 있는 방법을 사용해서 데이터셋을 불러올 수 있다.

``` python
from datasets import load_dataset

# 이 코드는 실행되는데 약간의 시간이 소요되기 때문에, 잠시 차나 커피를 마시면서 기다려보아요~
data_files = "https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset
```

``` python
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
```

위의 결과를 보면, 15,518,009개의 행과 2개의 열로 이루어진 데이터셋이라는 것을 알 수 있다.
진짜 엄청 많다!! 😲

첫 번째 example을 꺼내서 한 번 확인해보도록 하자.

``` python
pubmed_dataset[0]
```

``` python
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

 오케이! 이 example은 medical 기사의 abstract처럼 보인다.
 이제 이 데이터셋을 불러오기 위해 얼만큼의 RAM을 사용하였는지 알아보도록 하자.
 
 ### memory mapping의 마술 🪄
 
 Python에서 메모리 사용량을 측정하는 간단한 방법은 psutil 라이브러리를 사용하는 것이다.
 이 라이브러리는 pip을 이용하여 다음과 같이 설치할 수 있다.
 
 ``` python
 !pip install psutil
 ```
 
 이 라이브러리는 Process 클래스를 제공하는데, 이 클래스는 현재 프로세스가 메모리를 얼마나 사용하는지 확인해준다.
 
 ``` python
 import psutil
 
 # Process.memory_info는 byte로 표현되서, megabyte로 변환하였다.
 print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")
 ```
 
 ``` python
 RAM used: 5678.33 MB
 ```
 
 여기서 rss는 resident set size의 줄임말로, 프로세스가 RAM에서 차지하는 메모리의 일부이다.
 이러한 측정은 Python Interpreter가 사용하는 메모리와 라이브러리를 불러오는 메모리도 포함하기 때문에, 실제로 데이터셋을 불러오는데 사용된 메모리는 더욱 적다.
 비교를 위해, dataset_size를 사용해서 디스크에 있는 데이터셋이 얼마나 큰 지 알아보았다.
 결과는 이전과 마찬가지로 byte로 표현되기 때문에, 이를 gigabyte로 수동으로 바꿔야 한다.
 
 ``` python
 print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
 ```
 
 ``` python
Number of files in dataset : 20979437051
Dataset size (cache file) : 19.54 GB
 ```
 
 20 GB에 달하는 용량을 가지고 있는데, 데이터셋에 더욱 적은 양의 RAM을 사용해서 데이터셋을 불러오고 접근할 수 있었다.
 
 🤗 Datasets는 각각의 데이터셋을 라이브러리가 데이터셋을 메모리에 완전히 로드하지 않고도 데이터셋의 요소에 접근하고 작동할 수 있도록 하는 RAM과 파일 시스템 저장소 간의 매핑을 제공하는 memory-mapped file로 다룬다.

Memory-mapped file은 여러 개의 프로세스에 걸쳐서 공유될 수 있는데, 이는 method가 Dataset.map()처럼 데이터셋을 없애거나 옮길 필요없이 병렬화될 수 있도록 해준다.
이러한 기능은 모두 Apache Arrow 메모리 형식 및 pyarrow 라이브러리에 의해 실현되어 데이터 로드 및 처리 속도가 매우 빠르다.
이것이 작동하는 것을 보기 위해, PubMed Abstracts 데이터셋의 모든 요소를 반복함으로써 자그마한 스피드 테스트를 해보도록 하자.

``` python
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
```

``` python
'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'
```

여기서 Python의 timeit 모듈을 사용하여 code_snippet에 걸리는 실행 시간을 측정하였다.
보통 수십 GB/s에서 몇 GB/s의 속도로 데이터셋에 대해 반복할 수 있다.
이러한 작업은 광대한 다수의 응용에 좋지만, 가끔은 데이터셋이 너무 커서 하드 드라이브에 저장할 수 없는 경우도 있다.
예를 들어, Pile 데이터셋을 통째로 다운 받게된다면, 825 GB의 공간이 필요하다! 😵 
이러한 경우를 다루기 위해서, 🤗 Datasets는 전체 데이터셋을 다운할 필요없이, 즉석에서 다운하고 접근할 수 있게 해주는 streaming feature을 제공한다.

### Streaming datasets

dataset streaming을 가능하게 하기 위해서는, streaming=True를 load_dataset() 함수에 흘려보내면 된다.
예를 들어, PubMed Abstracts 데이터셋을 다시 불러와보자. 하지만, streaming mode에서 말이다.

``` python
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

이번 챕터에서 다뤘던 익숙한 Dataset 대신에, streaming=True로 반환된 object는 IterableDataset이다.
이름에서부터 알 수 있듯이, IterableDataset의 요소들에 접근하기 위해서는 이를 반복해야 할 필요가 있다.
우리의 streamed dataset의 첫 번째 요소를 다음과 같이 확인할 수 있다.

``` python
next(iter(pubmed_dataset_streamed))
```

``` python
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

streamed dataset의 요소들은 IterableDataset.map()을 사용해서 즉석에서 처리될 수 있다.
IterableDataset.map()은 학습 중에 input을 토큰화해야 할 때 유용하다.
처리 과정은 Chapter 3. 에서 사용한 토큰화와 완벽히 똑같다.
유일하게 다른 점은 output이 하나로 반환된다는 것이다.

``` python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

``` python
{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}
```

streamed dataset을 IterableDataset.shuffle()을 사용하여 셔플할 수 있지만, Dataset.shuffle()과 달리 이것은 사전에 정의도니 buffer_size의 요소만 셔플한다.

``` python
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

``` python
{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}
```

이 예시에서, 버퍼의 첫 10,000개의 example로부터 랜덤하게 example을 선택하게 한다.
example에 대해 접근이 되면, 버퍼에서의 이것의 spot은 corpus의 다음 example로 채워진다.
그리고 IterableDataset.take()와 IterableDataset.skip() 함수를 사용해서 streamed dataset로부터 요소들을 선택해올 수 있다.
이 방법은 Dataset.select() 와 유사하다.
예를 들어, PubMed Abstracts 데이터셋의 첫 5개의 example을 고르려면 다음과 같이 하면 된다.

``` python
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)
```

``` python
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

이와 유사하게, IterableDataset.skip() 함수를 사용해서 shuffled dataset에서 training과 validation split을 다음과 같이 만들 수 있다.

``` python
# 첫 1,000개의 example은 skip하고, 남은 example을 training set로 받음
train_dataset = shuffled_dataset.skip(1000)
# 첫 1,000개의 example을 validation set으로 받음
validation_dataset = shuffled_dataset.take(1000)
```

공통 application을 사용하여 데이터셋 스트리밍에 대한 탐색을 마무리하겠다.
그러기 위해, 일단 여러 데이터셋을 함께 결합하여 단일 corpus를 생성한다.
🤗 Datasets는 IterableDataset object 리스트를 하나의 IterableDataset으로 변환하는 interleave_datasets() 함수를 제공한다.
여기서 새로운 데이터셋의 요소들은 소스 example을 번갈아 가며 사용함으로써 얻어진다.
이러한 함수는 커다란 데이터셋을 합칠 때 특히 유용하게 사용된다.
예제로 Pile의 FreeLaw 서브셋을 stream해보도록 하자.

``` python
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

``` python
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

이 데이터셋은 로드하는데만 보통의 노트북에서는 RAM에 엄청난 스트레스를 줘야할 만큼 크지만, 우리는 어렵지 않게 데이터셋을 불러오고 접근할 수 있었다! 😊
그러면 이제 FreeLaw와 PubMed Abstracts 데이터셋의 example을 interleave_datasets() 함수로 합쳐보도록 하자.

``` python
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```

``` python
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]
```

여기서 Python의 itertools 모듈의 islice() 함수를 사용해서 통합 데이터셋의 첫 두 개 example을 선택하게 하고, 두 소스 데이터셋 각각의 첫 번째 예와 일치하는 것을 볼 수 있다.

마지막으로 파일 전체를 825 GB로 스트리밍하려면 다음과 같이 준비된 모든 파일을 가져올 수 있다.

``` python
base_url = "https://the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

``` python
{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web...'}
```
