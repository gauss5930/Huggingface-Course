# Transformer는 어떻게 작동할까?

이 절에서는 Transformer model의 architecture에 더욱 자세하게 들여다보았다.

### Transformer의 역사

다음의 그림은 Transformer model에 대한 간략한 역사를 보여준다. 
Transformer architecture는 2017년에 소개된 모델로 다음과 같은 후속 모델들의 베이스가 되었다.

![transformer 역사](https://user-images.githubusercontent.com/80087878/217529621-30047b8c-2558-41eb-b45c-5c7126b0521b.svg)

수많은 모델들이 존재하는데, 이를 좀 더 이해하기 쉽게 널리 사용되는 3가지 카테고리로 분류하면 다음과 같이 표현할 수 있다.

- GPT기반 or *auto-regressive* Transformer model
- BERT기반 or *auto-encoding* Transformer model
- BART/T5기반 or *seq-to-seq* Transformer model

### Transformer는 Language Model이다!

위에 언급한 모든 Transformer model은 *language model*로 학습되었다. 
이는 이들이 비지도학습 방식으로 거대한 양의 raw text로부터 학습되었다는 것을 의미한다.

이러한 방식으로 학습된 모델은 언어에 대해 통계학적 이해는 발전시키지만, 특정한 실용적인 task에 대해서는 그리 유용하지 않다.
그래서, 일반적인 pre-trained model은 *transfer learning*이라는 과정을 거치게 된다. 
이 과정 중에는 모델이 사람의 라벨을 이용하여 지도 학습 방식으로 주어진 task에 대해 fine-tuned된다.

이에 대한 예시로는 이전의 n개의 단어와 현재 단어를 이용하여 문장에 다음으로 나오게 될 단어를 예측하는 task가 있다.
이러한 방식을 *casual language modeling*이라고 하는데, 왜냐하면 출력이 미래가 아닌 과거와 현재의 입력에 의해 결정되기 때문이다.
다음의 그림은 *casual language modeling*의 예시이다.

![casual language modeling](https://user-images.githubusercontent.com/80087878/217532902-78b48ec1-3621-40fd-9d86-d42194be0fcc.svg)

또다른 예시로는 *masked language modeling*이 있다. 
이 방식은 모델이 문장의 중간에 mask가 씌워진 단어를 예측하는 방식으로 작동한다.
다음의 그림은 *masked language modeling*의 예시이다.

![masked language modeling](https://user-images.githubusercontent.com/80087878/217533225-819f1eae-ec2f-41de-b233-bae5a721a481.svg)

### Transformer는 거대한 모델!

특별한 케이스인 DistilBERT를 제외하면, 좋은 성능을 얻기 위한 일반적인 전략은 pre-trained하는 데이터의 양 뿐만 아니라, 모델의 크기를 키우는 것이다.
하지만, 이렇게 모델의 크기를 늘리고, 더 많은 양의 데이터를 요구로 하는 것은 엄청난 양의 자원을 요구한다. 

따라서 이렇게 거대한 모델을 사용할 때, 처음부터 다시 만들어서 사용하는 것은 너무 많은 자원을 요구로 한다.
이러한 불상사를 막기 위해 다음과 같은 방법이 제안되었고, 실제로도 많이 사용된다.

### Transfer Learning

*Pretraining*은 모델을 처음부터 학습시키는 것이다. 
초기에 가중치는 랜덤으로 설정되어 있고, 아무런 사전 지식 없이 학습은 시작된다.

![pretraining](https://user-images.githubusercontent.com/80087878/217986464-f5d649de-5be6-4168-8434-487c003d9720.svg)

이러한 pretraining은 보통 엄청난 양의 데이터에서 진행된다. 
그래서, 많은 양의 corpus 데이터를 필요로 하고, 오랜 시간이 걸린다.

반면에, *Fine-tuning*은 모델이 pretrained된 후에 진행된다.
fine-tuning을 수행하기 위해서, 우선 첫 번째로 pretrained 모델을 가져온 후, task에 맞는 specific한 데이터를 사용하여 추가적인 학습을 진행해야 한다.
여기서 "왜 한번에 최종 task를 수행할 수 있도록 학습시키지 않는 거지?" 라는 의문이 생길 수 있는데, 이에 대한 이유는 다음과 같다.

- pretrained 모델은 fine-tuning dataset과 유사한 데이터셋으로 이미 학습되어 있다. 따라서, fine-tuning 과정은 초기에 모델이 pretrain될 때 얻은 지식에 이점을 가질 수 있다. (예를 들어서, NLP 문제에 대해, pretrained 모델은 language에 대해 task에 필요한 통계학적 이해를 가지고 있다.)
- pretrained 모델은 이미 방대한 양의 데이터로 학습이 되었기 때문에, fine-tuning은 그에 준하는 성능을 얻기 위해 더욱 적은 양의 데이터를 요구한다.
- 이와 같은 이유로, 좋은 성능을 얻기 위한 시간과 자원이 현저히 줄어들 수 있기 때문이다.

Fine-tuning은 한정된 양의 데이터를 필요로 한다: pretrained 모델이 획득한 지식은 'transferred(전이)'되기 떄문에, *transfer learning*이라고 불리는 것이다.

![finetuning](https://user-images.githubusercontent.com/80087878/217988103-38beff29-d23c-4713-ac97-f5fbabcc6684.svg)

따라서, Fine-tuning은 더욱 적은 시간, 데이터, 경제적 그리고 환경적 비용이 든다. 그리고 빠르기도 한데다, 다양한 fine-tuning scheme에 쉽게 사용될 수 있다. 

이러한 과정은 처음부터 학습시키는 것보다 더 나은 결과를 얻을 수 있다. 그렇기 때문에 항상 pretrained 모델을 활용하고 fine-tuning해야 한다.

### Architecture

이 절에서는 Transformer 모델의 일반적인 architecture에 대해서 알아보았다.

#### Introduction

Transformer 모델은 주로 두 개의 블록으로 구성되어 있다.

- **Encoder**(왼쪽): encoder는 입력을 받아서 이 입력의 representation을 만든다. 이 말인 즉슨, 모델이 입력으로부터 정보를 얻기 위해 최적화된다는 의미이다.
- **Decoder**(오른쪽): decoder는 encoder의 representation과 또 다른 입력을 사용하여 target sentence를 생성한다. 이 말인 즉슨, 모델이 output을 생성하기 위해 최적화된다는 의미이다.

![transformers_blocks](https://user-images.githubusercontent.com/80087878/217999062-04aa393a-2904-43f5-ad18-68f3946699f7.svg)

각각의 파트는 독립적으로 사용되기도 하는데, Encoder-only 모델은 문장의 이해를 도와줌으로써 분류와 같은 task에 주로 사용되고, Decoder-only 모델은 text 생성에 주로 사용된다.

### Attention Layers

Transformer의 중요한 특징은 *attention layer*라고 불리는 특별한 layer을 사용한다는 것이다. attention에 대해 간략하게 설명하면, 모델이 문장 내에서 어떠한 특정한 단어에 집중해야할 지를 구하는 것이다. Attention이 어떤 건지 대충 알았으니, Transformer architecture에 대해 더욱 자세하게 들여다보자.

### Original Architecture

Transformer architecture는 원래 번역을 위해 디자인 되었다. 학습 중에 encoder는 특정 언어로 이루어진 sentence를 받고, decoder는 똑같은 문장을 목표 언어의 형태로 받게 된다. encoder에서 attention layer들은 sentence내의 모든 단어를 사용할 수 있다. 하지만, decoder는 순차적으로 작동하기 때문에, 이미 번역된 문장의 단어에만 attention할 수 있다. 예를 들어서, 번역 목표 중 3개의 단어를 예측했고, 이들을 decoder에 전달된다. 그리고 decoder는 encoder로부터 얻은 입력을 사용하여 4번째 단어를 예측한다.

이러한 과정을 학습 중에 빠르게 진행하기 위해, decoder는 전체 목표 sentence를 입력으로 받지만, 미래의 단어를 사용하는 것은 허락되지 않는다. 예를 들어서, 4번째 단어를 예측할 때, 1~3번째 단어만을 참고할 수 있다는 것이다.

original Transformer의 architecture는 다음의 그림과 같다. 왼쪽이 Encoder이고, 오른쪽이 Decoder이다.

![transformers](https://user-images.githubusercontent.com/80087878/218000654-17141e8d-20cf-4beb-8638-609ed28324f3.svg)

decoder의 첫 번째 attention layer는 decoder의 입력에 attention을 하지만, 두 번째 attention layer는 encoder의 output을 사용한다. 이것이 모든 입력에 대해 접근을 함에 따라 현재의 단어를 최대한 잘 예측하게 도와준다.

모델이 특별한 단어에 attention하는 것을 방지하기 위해 encoder와 decoder에서 *attention mask*가 사용될 수 있다. 예를 들어서, 입력에 사용되는 특별한 padding word와 같은 단어들에 말이다.

### Architecture vs. Checkpoints

Transformer 모델 course를 진행하기 전에, *model*만큼이나 *architecture*와 *checkpoints*라는 단어도 많이 듣게 될 것이다. 각각의 단어는 살짝씩 다른 의미를 내포하고 있다.

- **Architecture**: 모델의 뼈대 구조를 의미. 모델에서 진행되는 각각의 layer와 각각의 연산에 대한 정의
- **Checkpoints**: architecture가 주어지면 로드되는 가중치 값
- **Model**: 'architecture'나 'checkpoints' 둘 다 될 수 있음. 하지만 모호성을 없애기 위해 미리 명시함.
