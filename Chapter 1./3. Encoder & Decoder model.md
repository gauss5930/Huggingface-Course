# Encoder & Decoder models

Transformer의 Encoder와 Decoder는 많은 model들에 사용되고 있다. 그 중에 몇 개를 살펴보았다.

### Encoder models

Encoder model은 Transformer의 encoder만을 사용하는 모델이다. 
각각의 단계에서, attention layer는 최초 sentence의 모든 단어들에 접근할 수 있다.
이러한 모델들은 보통 'bi-directional'한 attention을 사용하는데, 이러한 모델을 *auto-encoding models*라 한다.

이러한 모델의 pretraining은 주어진 sentence에 어느 정도 손상을 가하고, 최초의 sentence로 다시 복구시키는 task를 수행한다.

Encoder model은 문장에 대한 이해를 요구하는 문장 분류와 같은 task에 적합하다.

Encoder model에는 ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa 등이 있다.

### Decoder models

Decoder model은 Transformer의 decoder만을 사용하는 모델이다.
각각의 단계에서, 단어가 주어지면, attention layer는 문장에서 그 단어 이전의 단어들에만 접근할 수 있다.
이러한 모델은 *auto-regressive model*이라 한다.

이러한 모델은 text 생성과 같은 task에 적합하다.

Decoder model에는 CTRL, GPT, Transformer XL 등이 있다.
