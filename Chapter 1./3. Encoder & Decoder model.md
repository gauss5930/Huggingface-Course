# Encoder & Decoder & Seq-to-seq models

Transformer의 Encoder와 Decoder는 많은 model들에 사용되고 있다. 그 중에 몇 개를 살펴보았다.

### Encoder models

Encoder model은 Transformer의 encoder만을 사용하는 모델이다. 
각각의 단계에서, attention layer는 최초 sentence의 모든 단어들에 접근할 수 있다.
이러한 모델들은 보통 'bi-directional'한 attention을 사용하는데, 이러한 모델을 *auto-encoding models*라 한다.

이러한 모델의 pretraining은 주어진 sentence에 어느 정도 손상을 가하고, 최초의 sentence로 다시 복구시키는 task를 수행한다.

Encoder model은 문장에 대한 이해를 요구하는 문장 분류와 같은 task에 적합하다.

Encoder model에는 ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa 등이 있다.

### Decoder models

Decoder model은 Transformer의 decoder만을 사용하는 모델이다.
각각의 단계에서, 단어가 주어지면, attention layer는 문장에서 그 단어 이전의 단어들에만 접근할 수 있다.
이러한 모델은 *auto-regressive model*이라 한다.

이러한 모델은 text 생성과 같은 task에 적합하다.

Decoder model에는 CTRL, GPT, Transformer XL 등이 있다.

### Seq-to-Seq models

Encoder-decoder model은 Transformer architecture의 두 부분을 모두 사용한다.
각각의 단계에서, 작동하는 attention layer는 Encoder model과 Decoder model의 attention layer와 똑같다.

이러한 모델의 pretraining은 encoder 또는 decoder model의 목표를 사용하여 끝날 수 있지만, 보통 더욱 복잡하게 진행된다.

Seq-to-Seq model은 입력이 주어졌을 때, 입력에 의존하여 새로운 sentence를 생성하는 task에 적합하다. 예를 들어, 요약, 번역, QA 등과 같은 task 말이다.

Seq-to-Seq model에는 BART, mBART, Marian, T5 등이 있다.
