# End-of-Chapter Quiz

Chapter 1.을 마무리하면서 그간 배운 내용들을 토대로 간단한 퀴즈를 풀어보자! 
각 문제의 답은 마지막에 있습니다.

#### 1. Hub에서 'roberta-large-mnli'의 checkpoint를 찾아보세요. 이 모델은 어떤 task를 수행했을까요?

- Summarization(요약)
- Text classification(분류)
- Text generation(생성)

#### 2. 다음의 코드가 내놓을 결과는?

``` python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

- 문장이 '긍정'인지 '부정'인지에 대한 값을 나타내서 분류
- 이 문장을 완성시킬 문장 '생성'
- 사람, 기관, 장소 중 어디에 속하는 단어인지 출력

#### 3. 코드에서 '...' 부분을 무엇으로 바꿔야 할까?

``` python
from transformers import pipeline

filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
```

- This <mask> has been waiting for you.
- This [MASK] has been waiting for you.
- This man has been waiting for you.
  
#### 4. 왜 이 코드가 실행되지 않을까?

``` python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
```
  
- 이 pipeline은 text를 분류하기 위해 주어지는 label를 필요로 한다.
- 이 pipeline은 한 개가 아닌, 여러 개의 문장을 필요로 한다.
- 🤗 Transformer 라이브러리가 고장났다!
- 이 pipeline은 더욱 긴 입력을 필요로 한다. 이건 너무 짧잖아!
  
#### 5. 'transfer learning'의 의미는?
  
- 똑같은 데이터셋에서 학습시키기 위해 pretrained 모델의 지식을 새로운 모델로 전이시킨다.
- 첫 번째 모델의 가중치로 두 번째 모델도 작동시키기 위해 pretrained 모델의 지식을 새로운 모델로 전이시킨다.
- 두 번째 모델도 첫 번째 모델의 architecture와 똑같이 만들기 위해 pretrained 모델의 지식을 새로운 모델로 전이시킨다.
  
#### 6. language model은 pretraining할 때, 보통 label이 필요없다.
  
- True
- False
  
#### 7. 'model', 'architecture', 'weights'를 가장 잘 설명한 문장을 고르시오.
  
- model이 지어지면, architecture가 이 모델의 청사진이고, weights는 그 안에 사는 사람들이다.
- architecture는 model을 만들기 위한 지도 같은 존재이고, weights는 지도에 표시된 도시 같은 존재이다.
- architecture는 model을 만들기 위한 수학적 함수의 연속이고, weights는 그러한 함수 파라미터이다.
  
#### 8. 문장 생성을 위해 어떤 모델을 사용해야 할까?
  
- Encoder model
- Decoder model
- Seq-to-Seq model
  
#### 9. 문장 요약을 위해 어떤 모델을 사용해야 할까?
  
- Encoder model
- Decoder model
- Seq-to-Seq model
  
#### 10. 특정 label에 따라 문장 분류 위해 어떤 모델을 사용해야 할까?
  
- Encoder model
- Decoder model
- Seq-to-Seq model

  
  
  
  
  
정답
1. 2
2. 3
3. 2
4. 1
5. 2
6. True
7. 3
8. 2
9. 3
10. 1
